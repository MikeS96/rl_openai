{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Official documentaiton Mountain car](https://gym.openai.com/envs/MountainCar-v0/)\n",
    "\n",
    "[Github source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py)\n",
    "\n",
    "[OpenAI Gym DOC](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiles3 as tc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP Process and the definition of each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **observation** (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "- **reward** (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "- **done** (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "- **info** (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/MDP.png\" width=\"380\" />\n",
    "\n",
    "[Section 3.1 of the textbook](http://www.incompleteideas.net/book/RLbook2018.pdf#page=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "\n",
    "     Type:  Box(2)\n",
    "     Num    Observation               Min            Max\n",
    "     0      Car Position              -1.2           0.6\n",
    "     1      Car Velocity              -0.07          0.07\n",
    "         \n",
    "**Actions**:\n",
    "\n",
    "     Type: Discrete(3)\n",
    "     Num    Action\n",
    "     0      Accelerate to the Left\n",
    "     1      Don't accelerate\n",
    "     2      Accelerate to the Right\n",
    "\n",
    "     Note: This does not affect the amount of velocity affected by the gravitational pull acting on the car\n",
    "        \n",
    "**Reward**:\n",
    "\n",
    "     Reward of 0 is awarded if the agent reached the flag(position = 0.5) on top of the mountain\n",
    "     Reward of -1 is awarded if the position of the agent is less than 0.5\n",
    "        \n",
    "**Starting State**:\n",
    "\n",
    "     The position of the car is assigned a uniform random value in [-0.6 , -0.4]\n",
    "     The velocity of the car is always assigned to 0\n",
    "        \n",
    "**Episode Termination**:\n",
    "\n",
    "     The car position is more than 0.5\n",
    "     Episode length is greater than 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Action Space is an object of type: Discrete(3)\n",
      "\n",
      "The shape of the action space is: 3\n",
      "\n",
      "The Environment Space is an object of type: Box(2,)\n",
      "\n",
      "The Shape of the dimension Space are: (2,)\n",
      "\n",
      "The High values in the observation space are [0.6  0.07], the low values are [-1.2  -0.07]\n",
      "\n",
      "The minimum and maximum car's position are: -1.2000000476837158, 0.6000000238418579\n",
      "\n",
      "The minimum and maximum car's velocity are: -0.07000000029802322, 0.07000000029802322\n",
      "\n",
      "The Observations at a given timestep are [-0.84685916  0.04859914]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "observation = env.reset() \n",
    "\n",
    "# Object's type in the action Space\n",
    "print(\"The Action Space is an object of type: {0}\\n\".format(env.action_space))\n",
    "# Shape of the action Space\n",
    "print(\"The shape of the action space is: {0}\\n\".format(env.action_space.n))\n",
    "# Object's type in the Observation Space\n",
    "print(\"The Environment Space is an object of type: {0}\\n\".format(env.observation_space))\n",
    "# Shape of the observation space\n",
    "print(\"The Shape of the dimension Space are: {0}\\n\".format(env.observation_space.shape))\n",
    "# The high and low values in the observation space\n",
    "print(\"The High values in the observation space are {0}, the low values are {1}\\n\".format(\n",
    "    env.observation_space.high, env.observation_space.low))\n",
    "# Minimum and Maximum car position\n",
    "print(\"The minimum and maximum car's position are: {0}, {1}\\n\".format(\n",
    "    env.observation_space.low[0], env.observation_space.high[0]))\n",
    "# Minimum and Maximum car velocity\n",
    "print(\"The minimum and maximum car's velocity are: {0}, {1}\\n\".format(\n",
    "    env.observation_space.low[1], env.observation_space.high[1]))\n",
    "# Example of observation\n",
    "print(\"The Observations at a given timestep are {0}\\n\".format(env.observation_space.sample()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile Coding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tile coding is introduced in [Section 9.5.4 of the textbook](http://www.incompleteideas.net/book/RLbook2018.pdf#page=239) of the textbook as a way to create features that can both provide good generalization and discrimination. It consists of multiple overlapping tilings, where each tiling is a partitioning of the space into tiles.\n",
    "\n",
    "<img src=\"./assets/tilecoding.png\" width=\"640\" />\n",
    "\n",
    " [Tiles3 documentation](http://incompleteideas.net/tiles/tiles3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile Coding Class\n",
    "class MountainCarTileCoder:\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        \"\"\"\n",
    "        Initializes the MountainCar Tile Coder\n",
    "        Initializers:\n",
    "        iht_size -- int, the size of the index hash table, typically a power of 2\n",
    "        num_tilings -- int, the number of tilings\n",
    "        num_tiles -- int, the number of tiles. Here both the width and height of the\n",
    "                     tile coder are the same\n",
    "        Class Variables:\n",
    "        self.iht -- tc.IHT, the index hash table that the tile coder will use\n",
    "        self.num_tilings -- int, the number of tilings the tile coder will use\n",
    "        self.num_tiles -- int, the number of tiles the tile coder will use\n",
    "        \"\"\"\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, position, velocity):\n",
    "        \"\"\"\n",
    "        Takes in a position and velocity from the mountaincar environment\n",
    "        and returns a numpy array of active tiles.\n",
    "        \n",
    "        Arguments:\n",
    "        position -- float, the position of the agent between -1.2 and 0.5\n",
    "        velocity -- float, the velocity of the agent between -0.07 and 0.07\n",
    "        returns:\n",
    "        tiles - np.array, active tiles\n",
    "        \"\"\"\n",
    "        # Set the max and min of position and velocity to scale the input\n",
    "        # The max position is set to 0.5 as this is the position to end the experiment\n",
    "        POSITION_MIN = -1.2\n",
    "        POSITION_MAX = 0.5\n",
    "        VELOCITY_MIN = -0.07\n",
    "        VELOCITY_MAX = 0.07\n",
    "        \n",
    "        # Scale position and velocity by multiplying the inputs of each by their scale\n",
    "        position_scale = self.num_tiles / (POSITION_MAX - POSITION_MIN)\n",
    "        velocity_scale = self.num_tiles / (VELOCITY_MAX - VELOCITY_MIN)\n",
    "        \n",
    "        # Obtain active tiles for current position and velocity\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [position * position_scale, \n",
    "                                                      velocity * velocity_scale])\n",
    "        \n",
    "        return np.array(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tiles obtained are: [0 1 2 3 4 5 6 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the TileCoder class\n",
    "mctc = MountainCarTileCoder(iht_size = 1024, num_tilings = 8, num_tiles = 8)\n",
    "tiles = mctc.get_tiles(position = -1.0, velocity = 0.01)\n",
    "# Tiles obtained at a random pos and vel\n",
    "print(\"The Tiles obtained are: {0}\\n\".format(tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return np.random.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Sarsa Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation: \n",
    "\n",
    "\\begin{equation} \n",
    "w \\leftarrow w + \\alpha[R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, w)- \\hat{q}(S_t, A_t, w)]\\nabla \\hat{q}(S_t, A_t, w)\n",
    "\\end{equation}\n",
    "\n",
    "Target:\n",
    "\n",
    "\\begin{equation} \n",
    "\\delta \\leftarrow R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, w)\n",
    "\\end{equation}\n",
    "\n",
    "Action-value with function approximation using Sarsa Algorithm\n",
    "\n",
    "<img src=\"./assets/pseudocode.png\" width=\"480\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA\n",
    "class SarsaAgent():\n",
    "    \"\"\"\n",
    "    Initialization of Sarsa Agent. All values are set to None so they can\n",
    "    be initialized in the agent_init method.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent_info={}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        self.epsilon = None\n",
    "        self.gamma = None\n",
    "        self.iht_size = None\n",
    "        self.w = None\n",
    "        self.alpha = None\n",
    "        self.num_tilings = None\n",
    "        self.num_tiles = None\n",
    "        self.mctc = None\n",
    "        self.initial_weights = None\n",
    "        self.num_actions = None\n",
    "        self.previous_tiles = None\n",
    "\n",
    "    def agent_init(self, agent_info={}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.num_tilings = agent_info.get(\"num_tilings\", 8)\n",
    "        self.num_tiles = agent_info.get(\"num_tiles\", 8)\n",
    "        self.iht_size = agent_info.get(\"iht_size\", 4096)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.0)\n",
    "        self.gamma = agent_info.get(\"gamma\", 1.0)\n",
    "        self.alpha = agent_info.get(\"alpha\", 0.5) / self.num_tilings\n",
    "        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n",
    "        self.num_actions = agent_info.get(\"num_actions\", 3)\n",
    "        \n",
    "        # Initialize self.w to three times the iht_size. Recall this is because\n",
    "        # we need to have one set of weights for each action (Stacked values).\n",
    "        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n",
    "        \n",
    "        # Initialize self.mctc to the mountaincar verions of the  tile coder created\n",
    "        self.mctc = MountainCarTileCoder(iht_size = self.iht_size, \n",
    "                                         num_tilings = self.num_tilings, \n",
    "                                         num_tiles = self.num_tiles)\n",
    "\n",
    "    def select_action(self, tiles):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon greedy\n",
    "        Args:\n",
    "        tiles - np.array, an array of active tiles\n",
    "        Returns:\n",
    "        (chosen_action, action_value) - (int, float), tuple of the chosen action\n",
    "                                        and it's value\n",
    "        \"\"\"\n",
    "        action_values = []\n",
    "        chosen_action = None\n",
    "        \n",
    "        # Obtain action values for all actions (sum through rows)\n",
    "        action_values = np.sum(self.w[:, tiles], axis = 1)\n",
    "        \n",
    "        # Epsilon Greedy action selecion\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Select random action among the three posible actions\n",
    "            chosen_action = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            # Select the greedy action\n",
    "            chosen_action = argmax(action_values)\n",
    "        \n",
    "        return chosen_action, action_values[chosen_action]\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's env.reset() function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        # Current state\n",
    "        position, velocity = state\n",
    "        \n",
    "        # Obtain tiles activated at state cero\n",
    "        active_tiles = self.mctc.get_tiles(position = position, velocity = velocity)\n",
    "        # Select an action and obtain action values of the state\n",
    "        current_action, action_value = self.select_action(active_tiles)\n",
    "        \n",
    "        # Save action as last action\n",
    "        self.last_action = current_action\n",
    "        # Save tiles as previous tiles\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        \n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        # Current state\n",
    "        position, velocity = state\n",
    "\n",
    "        # Compute current tiles\n",
    "        active_tiles = self.mctc.get_tiles(position = position, velocity = velocity)\n",
    "        # Obtain new action and action value before updating actition values\n",
    "        current_action, action_value = self.select_action(active_tiles)\n",
    "        \n",
    "        # Update the Sarsa Target (delta)\n",
    "        target = reward + (self.gamma * action_value)\n",
    "        \n",
    "        # Compute last action values to update weights\n",
    "        last_action_val = np.sum(self.w[self.last_action][self.previous_tiles]) \n",
    "        \n",
    "        # As we are using tile coding, which is a variant of linear function approximation\n",
    "        # The gradient of the active tiles are one, otherwise cero.\n",
    "        grad = 1\n",
    "        self.w[self.last_action][self.previous_tiles] = self.w[self.last_action][self.previous_tiles] + \\\n",
    "            self.alpha * (target - last_action_val) * grad\n",
    "                \n",
    "        self.last_action = current_action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "\n",
    "        # There is no action_value used here because this is the end\n",
    "        # of the episode.\n",
    "        \n",
    "        # Compute delta\n",
    "        target = reward \n",
    "        # Compute last action value\n",
    "        last_action_val = np.sum(self.w[self.last_action][self.previous_tiles])\n",
    "        grad = 1\n",
    "        # Update weights\n",
    "        self.w[self.last_action][self.previous_tiles] = self.w[self.last_action][self.previous_tiles] + \\\n",
    "            self.alpha * (target - last_action_val) * grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  mp4list.sort()\n",
    "  for mp4 in mp4list:\n",
    "    print(mp4)\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "\n",
    "def wrap_env(env, k):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:00<00:05,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:00<00:04,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:00<00:03,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:01<00:03,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:01<00:02,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:01<00:02,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:01<00:02,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:02<00:01,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:02<00:01,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:02<00:01,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:02<00:01,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:03<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:03<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 181 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:03<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Sarsa Agent \n",
    "env_to_wrap = gym.make('MountainCar-v0')\n",
    "env = Monitor(env_to_wrap, \"./vid\", video_callable=lambda episode_id: True,force=True)\n",
    "\n",
    "num_runs = 30\n",
    "num_episodes = 200\n",
    "agent_info_options = {\"num_tilings\": 8, \"num_tiles\": 8, \"iht_size\": 4096,\n",
    "                      \"epsilon\": 0.0, \"gamma\": 1.0, \"alpha\": 0.5,\n",
    "                      \"initial_weights\": 0.0, \"num_actions\": 3}\n",
    "all_steps = []\n",
    "\n",
    "agent = SarsaAgent(agent_info_options)\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "agent.agent_init(agent_info_options)\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "# Number of timesteps the agent will run steps (a.k.a episodes)\n",
    "for i_episode in tqdm(range(num_runs)):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    # Reset agent\n",
    "    #agent.agent_init(agent_info_options)\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "    for t in range(200):\n",
    "        # View environment\n",
    "        env.render()\n",
    "        \n",
    "        # Take a step with the environment\n",
    "        observation, reward, done, info = env.step(last_action)\n",
    "        \n",
    "        # If the goal has been reached stop\n",
    "        if done:\n",
    "            # Last step with the agent\n",
    "            agent.agent_end(reward)\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        else:\n",
    "            # Take a step with the agent\n",
    "            last_action = agent.agent_step(reward, observation)\n",
    "\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
