{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Pendulum with function appoximation and control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is intended to solve the Continuous Pendulum problem using Policy gradient methods (Deep actor-critic).\n",
    "\n",
    "The description of the problem is given below:\n",
    "\n",
    "\"The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright..\" \n",
    "\n",
    "<img src=\"./assets/pendulum.png\" width=\"380\" />\n",
    "\n",
    "Image and Text taken from Taken from [Official documentaiton Pendulum](https://gym.openai.com/envs/Pendulum-v0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "As a quick recap, the diagram below explains the workflow of a Markov Decision Process (MDP)\n",
    "\n",
    "<img src=\"./assets/MDP.png\" width=\"380\" />\n",
    "\n",
    "Image taken from [Section 3.1 Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Agent specifications\n",
    "\n",
    "The states, actions, reward and termination are given as follows for the lunar lander problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "\n",
    "     Type:  Box(2)\n",
    "     Num \tObservation \t Min \tMax\n",
    "     0 \t  cos(theta)      -1.0 \t1.0\n",
    "     1 \t  sin(theta) \t -1.0 \t1.0\n",
    "     2       theta dot \t  -8.0 \t8.0\n",
    "         \n",
    "**Actions**:\n",
    "\n",
    "     Type: Box(1)\n",
    "     Num \tAction \t        Min \tMax\n",
    "     0 \t  Joint effort      -2.0 \t2.0\n",
    "\n",
    "        \n",
    "**Reward**:\n",
    "\n",
    "     -(theta^2 + 0.1*theta_dt^2 + 0.001*action^2)\n",
    "\n",
    "        \n",
    "**Starting State**:\n",
    "\n",
    "     Random angle from -pi to pi, and random velocity between -1 and 1\n",
    "        \n",
    "**Episode Termination**:\n",
    "\n",
    "     Continuous problem\n",
    "     \n",
    "For further information see [Github source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell aims to show how to iterate with the action and observation space of the agent and extract relevant information from it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Action Space is an object of type: Box(1,)\n",
      "\n",
      "The shape of the action space is: (1,)\n",
      "\n",
      "The High values in the action space are [2.], the low values are [-2.]\n",
      "\n",
      "The Environment Space is an object of type: Box(3,)\n",
      "\n",
      "The Shape of the dimension Space are: (3,)\n",
      "\n",
      "The High values in the observation space are [1. 1. 8.], the low values are [-1. -1. -8.]\n",
      "\n",
      "The Observations at a given timestep are [-0.20642476  0.39469406 -5.591796  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "observation = env.reset() \n",
    "\n",
    "# Object's type in the action Space\n",
    "print(\"The Action Space is an object of type: {0}\\n\".format(env.action_space))\n",
    "# Shape of the action Space\n",
    "print(\"The shape of the action space is: {0}\\n\".format(env.action_space.shape))\n",
    "# The high and low values in the action space\n",
    "print(\"The High values in the action space are {0}, the low values are {1}\\n\".format(\n",
    "    env.action_space.high, env.action_space.low))\n",
    "# Object's type in the Observation Space\n",
    "print(\"The Environment Space is an object of type: {0}\\n\".format(env.observation_space))\n",
    "# Shape of the observation space\n",
    "print(\"The Shape of the dimension Space are: {0}\\n\".format(env.observation_space.shape))\n",
    "# The high and low values in the observation space\n",
    "print(\"The High values in the observation space are {0}, the low values are {1}\\n\".format(\n",
    "    env.observation_space.high, env.observation_space.low))\n",
    "# Example of observation\n",
    "print(\"The Observations at a given timestep are {0}\\n\".format(env.observation_space.sample()))\n",
    "\n",
    "# https://medium.com/deeplearningmadeeasy/advantage-actor-critic-continuous-case-implementation-f55ce5da6b4c\n",
    "# https://www.coursera.org/learn/prediction-control-function-approximation/home/welcome\n",
    "# [Section 3.1 Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=357)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, the critic is a Neural Network with a single output. The input dimension is equal to the number of states in the problem. Recall that.\n",
    "\n",
    "$$ v_\\pi(s) \\approx \\hat{v}(s, w) = NN(s, w) $$\n",
    "\n",
    "For further information about Neural Networks for function approximation see [Section 9.7 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Neural Network\n",
    "class Critic(nn.Module):\n",
    "    # Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).\n",
    "    def __init__(self, critic_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of states\n",
    "        self.state_dim = critic_config.get(\"state_dim\")\n",
    "        # Hidden units\n",
    "        self.num_hidden_units = critic_config.get(\"num_hidden_units\")\n",
    "        \n",
    "        # Initialzie first hidden layer \n",
    "        self.hidden_1 = nn.Linear(self.state_dim, self.num_hidden_units)\n",
    "        # Initialzie second hidden layer \n",
    "        self.hidden_2 = nn.Linear(self.num_hidden_units, self.num_hidden_units)\n",
    "        # Initialize output layer\n",
    "        self.output = nn.Linear(self.num_hidden_units, 1)\n",
    "                        \n",
    "    \n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This is a feed-forward pass in the network\n",
    "        Args:\n",
    "            s (Numpy array): The state, a 2D array of shape (state_dim)\n",
    "        Returns:\n",
    "            The value estimates (Torch array) calculated using the network's weights.\n",
    "            A 2D array of shape (num_actions)\n",
    "        \"\"\"\n",
    "        # Transform observations into a pytorch tensor\n",
    "        s = torch.Tensor(s)\n",
    "        \n",
    "        v = F.relu(self.hidden_1(s))\n",
    "        v = F.relu(self.hidden_2(v))\n",
    "        v = self.output(v)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Neural Network\n",
    "\n",
    "As this is a contininuous problem, it is possible to use a Normal distribution to sample actions. For this task, a neural network is used to compute $\\mu$ and $\\sigma$ which are the mean and standard deviation of a Normal Distribution.\n",
    "\n",
    "$$ \\pi(a | s, \\theta) \\doteq \\frac{1}{\\sigma(s, \\theta) \\sqrt{2\\pi}}e^-{\\frac{(a-\\mu(s,\\theta))^2}{2\\sigma(s,\\theta)^2}} $$\n",
    "\n",
    "As $\\mu$ and $\\sigma$ are computed by the network, they are automatically adjusted depending on the current state of the problem. The input of the neural netork are the states of the problem, the output is composed of two nodes, ($\\mu$ and $\\sigma$) for a unique action in this problem.\n",
    "\n",
    "For further explanation see [Section 13.7 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actor neural network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,  actor_config):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Number of states\n",
    "        self.state_dim = actor_config.get(\"state_dim\")\n",
    "        # Hidden units\n",
    "        self.num_hidden_units = actor_config.get(\"num_hidden_units\")\n",
    "        # Actions or output units\n",
    "        self.num_actions = actor_config.get(\"num_actions\")\n",
    "        \n",
    "        # Initialzie first hidden layer \n",
    "        self.hidden_1 = nn.Linear(self.state_dim, self.num_hidden_units)\n",
    "        # Initialzie second hidden layer \n",
    "        self.hidden_2 = nn.Linear(self.num_hidden_units, self.num_hidden_units)\n",
    "        # Initialize output layer (muliply output by 2 to obtain mu and sigma)\n",
    "        self.output = nn.Linear(self.num_hidden_units, self.num_actions * 2)\n",
    "        \n",
    "        # Log of standard deviation\n",
    "        #logstdv_param = nn.Parameter(torch.full((self.num_actions,), 0.1))\n",
    "        # Register parameter in the network\n",
    "        #self.register_parameter(\"logstdv\", logstdv_param)\n",
    "                \n",
    "    def compute_mean(self, s):\n",
    "        \"\"\"\n",
    "        This is a feed-forward pass in the network\n",
    "        Args:\n",
    "            s (Numpy array): The state, a 2D array of shape (state_dim)\n",
    "        Returns:\n",
    "            Mean and stdv (Torch array) calculated using the network's weights.\n",
    "            A 2D array of shape (num_actions * 2)\n",
    "        \"\"\"\n",
    "        # Transform observations into a pytorch tensor\n",
    "        s = torch.Tensor(s)\n",
    "        \n",
    "        pi = F.relu(self.hidden_1(s))\n",
    "        pi = F.relu(self.hidden_2(pi))\n",
    "        pi = self.output(pi)\n",
    "        \n",
    "        return pi\n",
    "                \n",
    "    \n",
    "    def forward(self, s):\n",
    "        \n",
    "        # Compute the mean and log stedv with the model\n",
    "        coeff = self.compute_mean(s)\n",
    "        print(coeff)\n",
    "        mean = coeff[:,0]\n",
    "        logstdv = coeff[:,1]\n",
    "        print(mean)\n",
    "        print(logstdv)\n",
    "        #print(mean)\n",
    "        #logstdv \n",
    "        # Exp the log stdv (To avoid negative values)\n",
    "        #stdv = mean.exp()\n",
    "        \n",
    "        # Clamp the stdv between 1e-3 and 50\n",
    "        #stdv = torch.clamp(self.logstdv.exp(), 1e-3, 50)\n",
    "        \n",
    "        # Sample an action from the normal distribution\n",
    "        return torch.distributions.Normal(mean, stdv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        \"\"\"\n",
    "        # Create the buffer\n",
    "        self.buffer = [] \n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state of size (state_dim)            \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state of size (state_dim) .           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            # Delete first position of the buffer if the Queue size is equals to max size\n",
    "            del self.buffer[0]\n",
    "        # Append new step\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "            The return of this function is of size (minibatch_size)\n",
    "        \"\"\"\n",
    "        idxs = np.random.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TD target and TD estimate\n",
    "\n",
    "The TD target and TD estimate's computation will be done in the next lines. The main idea here is to obtain both TD values from the Critic's network.\n",
    "\n",
    "The TD error is composed of two values, the TD estimate and TD Target.\n",
    "\n",
    "$$ \\delta = [R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(s_t,w)] = \\text{TD error} $$\n",
    " \n",
    "Recall that the TD Target is given by.\n",
    "\n",
    "$$ R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) $$\n",
    "\n",
    "Similarly, the TD estimate is.\n",
    "\n",
    "$$ \\hat{v}(s_t,w) $$\n",
    "\n",
    "\n",
    "For further explanation about TD methods see [Section 6.1 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=141)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute the TD Target and TD estimate\n",
    "def get_td(states, next_states, rewards, terminals, critic, discount, current_c):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): States with shape (state_dim).\n",
    "        next_states (Numpy array): Next states with shape (state_dim).\n",
    "        rewards (float): Current reward.\n",
    "        terminals (float): Terminal state.\n",
    "        critic (Critic): The critic neural network used to compute value estimates\n",
    "        discount (float): The discount factor (gamma).\n",
    "    Returns:\n",
    "        target_vec (Tensor value): The TD Target.\n",
    "        estimate_vec (Tensor value): The TD estimate.\n",
    "    \"\"\"\n",
    "    # Compute value estimates for the next states\n",
    "    v_next_vals = current_c.forward(next_states)\n",
    "    # Compute value estimates for the current states (TD estimate)\n",
    "    v_curr_vals = critic.forward(states).squeeze()\n",
    "    \n",
    "    # Compute the TD Target\n",
    "    target_vec = torch.tensor(rewards) + (discount * v_next_vals.squeeze() * (1 - torch.tensor(terminals)))\n",
    "    \n",
    "    return target_vec, v_curr_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer_c, optimizer_a, actor, critic, criterion, current_c):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Return:\n",
    "        Loss (float): The loss value for the current batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states) # Batch per states\n",
    "    next_states = np.concatenate(next_states) # Batch per states\n",
    "    rewards = np.array(rewards) # Batch size\n",
    "    terminals = np.array(terminals) # Batch size\n",
    "    batch_size = states.shape[0] # Batch size\n",
    "    \n",
    "    # Computing TD target and estimate using get_td function\n",
    "    td_target, td_estimate = get_td(states, next_states, rewards, terminals, critic, discount, current_c)\n",
    "    \n",
    "    # TD error calculation\n",
    "    delta = td_target - td_estimate\n",
    "\n",
    "    # Critic\n",
    "    # zero the gradients buffer\n",
    "    optimizer_c.zero_grad()\n",
    "    # Compute the  Mean squared value error loss\n",
    "    critic_loss = criterion(td_estimate.double().to(device), td_target.double().to(device))\n",
    "    #critic_loss = delta**2\n",
    "    # Backprop the error\n",
    "    critic_loss.backward()\n",
    "    # Clip the gradients to 0.5\n",
    "    clip_grad_norm_(optimizer_c, 0.5)\n",
    "    # Optimize critic's network\n",
    "    optimizer_c.step()\n",
    "\n",
    "    # Actor \n",
    "    optimizer_a.zero_grad()\n",
    "    # Compute mu and sigma\n",
    "    norm_dists = actor.forward(states)\n",
    "    # Construct equivalent loss function\n",
    "    logs_probs = norm_dists.log_prob(torch.tensor(actions))\n",
    "    #entropy = norm_dists.entropy() # - entropy * 0.0001\n",
    "    # Multiply by minus one as this is gradient ascent\n",
    "    actor_loss = (-logs_probs * delta.detach()).mean()\n",
    "    # Backprop the error\n",
    "    actor_loss.backward()\n",
    "    # Clip the actor's gradients\n",
    "    clip_grad_norm_(optimizer_a, 0.5)\n",
    "    # Optimize the actor's network\n",
    "    optimizer_a.step()\n",
    "    \n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipping the gradients\n",
    "\n",
    "To guarantee stability in the training process, it is necessary to clip the gradients of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Actor-Critic Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to implement the Agent. The training process is cyclical as this is a continuous task and the sequence goes as follows.\n",
    "\n",
    "$\\delta_t = R_{t+1}  + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_{t}, \\mathbf{w}) \\hspace{6em}  (1)$\n",
    "\n",
    "**Critic weight update rule**: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^{\\mathbf{w}}\\delta\\nabla \\hat{v}(s,\\mathbf{w}) \\hspace{2.5em} (2)$\n",
    "\n",
    "**Actor weight update rule**: $\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\mathbf{\\theta}}\\delta\\nabla ln \\pi(A|S,\\mathbf{\\theta}) \\hspace{1.4em} (3)$\n",
    "\n",
    "For further details, see [Section 13.5 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=353)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Agent\n",
    "class ActorCritic():\n",
    "    \"\"\"\n",
    "    Initialization of Actor-Critic Agent. All values are set to None so they can\n",
    "    be initialized in the agent_init method.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.actor_step_size = None\n",
    "        self.critic_step_size = None\n",
    "        self.avg_reward_step_size = None\n",
    "\n",
    "        self.avg_reward = None\n",
    "        self.critic = None\n",
    "        self.actor = None\n",
    "\n",
    "        self.actions = None\n",
    "\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "    def agent_init(self, agent_config = {}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        \n",
    "        # Step size parameters\n",
    "        self.actor_step_size = agent_config['optimizers_config']['actor_step_size']\n",
    "        self.critic_step_size = agent_config['optimizers_config']['critic_step_size']\n",
    "        self.discount = agent_config['optimizers_config']['gamma']\n",
    "        \n",
    "        # Set number of actions\n",
    "        self.actions = agent_config['network_config']['num_actions']\n",
    "        \n",
    "        # Initialize actor and critic networks\n",
    "        self.actor = Actor(agent_config['network_config']).to(device)\n",
    "        self.critic = Critic(agent_config['network_config']).to(device)\n",
    "        \n",
    "        # Critic Loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(50000, 16)\n",
    "        \n",
    "        # Loss variables\n",
    "        self.actor_loss_val = 0\n",
    "        self.critic_loss_val = 0\n",
    "        \n",
    "        # Actor and Critic optimizer\n",
    "        self.actor_opti = optim.Adam(self.actor.parameters(), lr = self.actor_step_size ) \n",
    "        self.critic_opti = optim.Adam(self.critic.parameters(), lr = self.critic_step_size )\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon greedy\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            chosen_action (float)\n",
    "        \"\"\"\n",
    "        # Pass the states to create the normal distribution\n",
    "        dist = self.actor.forward(state)\n",
    "        # Sample an action from the current normal Distribution\n",
    "        action = dist.sample().detach().data.numpy()\n",
    "        # Clip action to a given range (Max and Min in action space)\n",
    "        #chosen_action = torch.clamp(action, env.action_space.low.min(), env.action_space.high.max())\n",
    "        chosen_action = np.clip(action, env.action_space.low.min(), env.action_space.high.max())\n",
    "        \n",
    "        #self.log_probs = dists.log_prob(torch.tensor(action).detach())\n",
    "        #m = nn.Tanh()\n",
    "        #chosen_action = m(action) * env.action_space.high.max()\n",
    "\n",
    "        return chosen_action\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's env.reset() function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        # Select an action given the current state\n",
    "        current_action = self.select_action(state)\n",
    "\n",
    "        # Save action as last action\n",
    "        self.last_action = current_action\n",
    "        # Save tiles as previous tiles\n",
    "        self.last_state = np.array([state])\n",
    "        \n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        # Select action\n",
    "        action = self.select_action(state) #change for state for submission, normally, it is self.last_state\n",
    "        state = np.array([state])\n",
    "\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            # Make a copy of the current network to obtain stable targets\n",
    "            current_c = deepcopy(self.critic)\n",
    "            for _ in range(4):                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                _, _ = optimize_network(experiences, self.discount, self.critic_opti, self.actor_opti, self.actor,\n",
    "                                 self.critic, self.critic_loss, current_c)\n",
    "                \n",
    "        \n",
    "        # Compute new action\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "\n",
    "        # There is no action_value used here because this is the end\n",
    "        # of the episode.\n",
    "        \n",
    "        # Create state full of zeroes\n",
    "        state = np.zeros_like(self.last_state)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            # Make a copy of the current network to obtain stable targets\n",
    "            current_c = deepcopy(self.critic)\n",
    "            for _ in range(4):                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                _, _ = optimize_network(experiences, self.discount, self.critic_opti, self.actor_opti, self.actor,\n",
    "                                 self.critic, self.critic_loss, current_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "The following lines solves the Pendulum problem and plot the average reward obtained over episodes and steps taken to solve the challenge at a specific episode (The experiment is over every x amount of steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:10<08:36, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 0 was: -3617.9298571927247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [00:21<08:32, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 1 was: -3816.0274775903395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/50 [00:29<07:40,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 2 was: -3689.4110237255845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/50 [00:40<07:52, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 3 was: -3037.5548121718957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [00:52<07:59, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 4 was: -2525.0526067022984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [01:03<07:57, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 5 was: -2830.015115243156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 7/50 [01:15<07:56, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 6 was: -2400.21113914936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 8/50 [01:27<07:54, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 7 was: -2616.9553552351317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 9/50 [01:34<06:58, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 8 was: -2549.5565357629016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 10/50 [01:46<07:02, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 9 was: -3026.2573650807244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 11/50 [01:57<06:57, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 10 was: -2828.0697786859837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [02:06<06:35, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 11 was: -2539.193673138179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 13/50 [02:14<05:51,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 12 was: -2564.5672141941777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 14/50 [02:21<05:18,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 13 was: -2380.115111554478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 15/50 [02:28<04:51,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 14 was: -2303.2560213811344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [02:36<04:36,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 15 was: -2495.7935548488695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [02:43<04:21,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 16 was: -2216.5959034673683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [02:51<04:07,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 17 was: -1856.9974287719554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 19/50 [02:59<04:08,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 18 was: -2440.229401135776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 20/50 [03:12<04:40,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 19 was: -1997.8551097766313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 21/50 [03:19<04:13,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 20 was: -2063.8722321140317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 22/50 [03:27<03:59,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 21 was: -1902.895405562651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 23/50 [03:39<04:19,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 22 was: -1647.0229436429076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 24/50 [03:51<04:31, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 23 was: -1803.9405050462171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 25/50 [04:02<04:18, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 24 was: -1804.490288945043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 26/50 [04:09<03:46,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 25 was: -1407.5985771248488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 27/50 [04:18<03:31,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 26 was: -1150.4164041550946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 28/50 [04:26<03:14,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 27 was: -914.0039936276885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 29/50 [04:36<03:12,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 28 was: -777.9770989784553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 30/50 [04:48<03:21, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 29 was: -725.3107426291463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 31/50 [04:55<02:57,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 30 was: -390.7392483266976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 32/50 [05:03<02:38,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 31 was: -744.2809679974462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 33/50 [05:10<02:22,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 32 was: -908.501383723642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 34/50 [05:18<02:10,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 33 was: -147.5779561559089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 35/50 [05:30<02:18,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 34 was: -834.4055070703423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 36/50 [05:41<02:19,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 35 was: -694.9129557875422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 37/50 [05:50<02:05,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 36 was: -521.6854902843743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 38/50 [06:02<02:03, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 37 was: -257.9911818155458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 39/50 [06:10<01:45,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 38 was: -8.580065926503547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 40/50 [06:18<01:30,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 39 was: -137.33378636510363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 41/50 [06:26<01:18,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 40 was: -399.33474329358563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 42/50 [06:34<01:07,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 41 was: -401.4092757384472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 43/50 [06:41<00:57,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 42 was: -136.05144766693232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 44/50 [06:49<00:48,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 43 was: -373.3804708964082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 45/50 [06:57<00:39,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 44 was: -269.60549457167116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 46/50 [07:04<00:31,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 45 was: -137.22647081112746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 47/50 [07:12<00:23,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 46 was: -383.61599925481903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 48/50 [07:19<00:15,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 47 was: -386.6863228083072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 49/50 [07:27<00:07,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 48 was: -654.982657901044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:35<00:00,  9.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 49 was: -536.2022873759133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the Actor-Critic agent\n",
    "\n",
    "num_runs = 1\n",
    "num_episodes = 50\n",
    "\n",
    "agent_info = {\n",
    "             'network_config': {\n",
    "                 'state_dim': env.observation_space.shape[0],\n",
    "                 'num_hidden_units': 64,\n",
    "                 'num_actions': env.action_space.shape[0]\n",
    "             },\n",
    "             'optimizers_config': {\n",
    "                 'actor_step_size': 4e-4,  #5e-6, 1e-5 \n",
    "                 'critic_step_size': 4e-3, \n",
    "                 'gamma': 0.9, \n",
    "             }}\n",
    "\n",
    "# Variable to store the amount of steps taken to solve the challeng\n",
    "all_steps = []\n",
    "# Variable to save the rewards in an episode\n",
    "all_rewards = []\n",
    "# Save actor and critic losses\n",
    "all_loss_c = []\n",
    "all_loss_a = []\n",
    "\n",
    "# Agent\n",
    "agent = ActorCritic()\n",
    "\n",
    "# Environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "# Maximum number of possible iterations (default was 200)\n",
    "env._max_episode_steps = 500\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "for n_runs in range(num_runs):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    # Reset agent\n",
    "    agent.agent_init(agent_info)\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "    # Steps, rewards and loss at each episode to solve the challenge\n",
    "    steps_per_episode = []\n",
    "    rewards_per_episode = []\n",
    "    loss_per_episode_c = []\n",
    "    loss_per_episode_a = []\n",
    "        \n",
    "    # Times the environment will start again without resetting the agent\n",
    "    for t in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # Reset done flag\n",
    "        done = False\n",
    "        # Set rewards, steps and loss to zero\n",
    "        rewards = 0\n",
    "        n_steps = 0\n",
    "        agent.actor_loss_val = 0\n",
    "        agent.critic_loss_val = 0\n",
    "        # Reset environment\n",
    "        observation = env.reset()\n",
    "        # Run until the experiment is over\n",
    "        while not done:\n",
    "            \n",
    "            # Render the environment \n",
    "            env.render()\n",
    "\n",
    "            # Take a step with the environment\n",
    "            observation, reward, done, info = env.step(last_action)\n",
    "            \n",
    "            rewards += reward.squeeze()\n",
    "            n_steps += 1\n",
    "\n",
    "            # If the goal has been reached stop\n",
    "            if done:\n",
    "                # Last step with the agent\n",
    "                agent.agent_end(reward)\n",
    "            else:\n",
    "                # Take a step with the agent\n",
    "                last_action = agent.agent_step(reward, observation)\n",
    "                \n",
    "        # Append steps taken to solve the episode\n",
    "        steps_per_episode.append(n_steps)\n",
    "        # Reward obtained during the episode\n",
    "        rewards_per_episode.append(rewards)\n",
    "        # Losses obtained solving the experiment\n",
    "        loss_per_episode_c.append(agent.critic_loss_val)\n",
    "        loss_per_episode_a.append(agent.actor_loss_val)\n",
    "        print(\"The reward obtained during episode {0} was: {1}\".format(t, rewards))\n",
    "\n",
    "\n",
    "    # Steps taken to solve the experiment during all\n",
    "    all_steps.append(np.array(steps_per_episode))\n",
    "    # Awards obtained during all episode\n",
    "    all_rewards.append(np.array(rewards_per_episode))\n",
    "    # Losses obtained during all episodes\n",
    "    all_loss_c.append(np.array(loss_per_episode_c))\n",
    "    all_loss_a.append(np.array(loss_per_episode_a))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_average = np.mean(all_rewards, axis=0)\n",
    "plt.plot(rewards_average, label = 'Average Reward')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\" ,rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(rewards_average.min(), rewards_average.max())\n",
    "plt.title(\"Average iterations to solve the experiment over runs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The best reward obtained solving the experiment was: {0}\\n\".format(np.array(all_rewards).max()))\n",
    "print(\"The WorSt reward obtained solving the experiment was: {0}\\n\".format(np.array(all_rewards).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_average = np.mean(np.array(all_loss_c), axis=0)\n",
    "plt.plot(loss_average, label = 'Steps')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average loss\",rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(loss_average.min(), loss_average.max())\n",
    "plt.title(\"Average loss over iterations (Critic)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The best loss obtained solving the experiment was: {0}\\n\".format(np.array(loss_average).min()))\n",
    "print(\"The Worst loss obtained solving the experiment was: {0}\\n\".format(np.array(loss_average).max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_average = np.mean(np.array(all_loss_a), axis=0)\n",
    "plt.plot(loss_average, label = 'Steps')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average loss\",rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(loss_average.min(), loss_average.max())\n",
    "plt.title(\"Average loss over iterations (Actor)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The best loss obtained solving the experiment was: {0}\\n\".format(np.array(loss_average).max()))\n",
    "print(\"The Worst loss obtained solving the experiment was: {0}\\n\".format(np.array(loss_average).min()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the last trained Agent \n",
    "\n",
    "This lines shows in a video the performance of the last trained agent and save a video with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1000 [00:00<02:09,  7.69it/s]\u001b[A\n",
      "  0%|          | 5/1000 [00:00<01:39, 10.04it/s]\u001b[A\n",
      "  1%|          | 10/1000 [00:00<01:16, 13.00it/s]\u001b[A\n",
      "  1%|▏         | 14/1000 [00:00<01:00, 16.29it/s]\u001b[A\n",
      "  2%|▏         | 18/1000 [00:00<00:49, 19.79it/s]\u001b[A\n",
      "  2%|▏         | 22/1000 [00:00<00:42, 23.11it/s]\u001b[A\n",
      "  3%|▎         | 26/1000 [00:00<00:37, 26.26it/s]\u001b[A\n",
      "  3%|▎         | 30/1000 [00:00<00:34, 28.52it/s]\u001b[A\n",
      "  3%|▎         | 34/1000 [00:00<00:31, 30.80it/s]\u001b[A\n",
      "  4%|▍         | 38/1000 [00:01<00:29, 32.31it/s]\u001b[A\n",
      "  4%|▍         | 42/1000 [00:01<00:29, 32.26it/s]\u001b[A\n",
      "  5%|▍         | 46/1000 [00:01<00:29, 32.16it/s]\u001b[A\n",
      "  5%|▌         | 50/1000 [00:01<00:33, 28.56it/s]\u001b[A\n",
      "  5%|▌         | 54/1000 [00:01<00:35, 26.52it/s]\u001b[A\n",
      "  6%|▌         | 57/1000 [00:01<00:35, 26.35it/s]\u001b[A\n",
      "  6%|▌         | 60/1000 [00:01<00:36, 25.99it/s]\u001b[A\n",
      "  6%|▋         | 63/1000 [00:02<00:36, 25.86it/s]\u001b[A\n",
      "  7%|▋         | 66/1000 [00:02<00:35, 26.13it/s]\u001b[A\n",
      "  7%|▋         | 70/1000 [00:02<00:33, 27.76it/s]\u001b[A\n",
      "  7%|▋         | 73/1000 [00:02<00:33, 27.47it/s]\u001b[A\n",
      "  8%|▊         | 76/1000 [00:02<00:34, 27.06it/s]\u001b[A\n",
      "  8%|▊         | 80/1000 [00:02<00:33, 27.65it/s]\u001b[A\n",
      "  8%|▊         | 83/1000 [00:02<00:36, 25.43it/s]\u001b[A\n",
      "  9%|▊         | 86/1000 [00:02<00:36, 24.93it/s]\u001b[A\n",
      "  9%|▉         | 89/1000 [00:03<00:36, 24.63it/s]\u001b[A\n",
      "  9%|▉         | 92/1000 [00:03<00:38, 23.79it/s]\u001b[A\n",
      " 10%|▉         | 95/1000 [00:03<00:37, 23.90it/s]\u001b[A\n",
      " 10%|▉         | 98/1000 [00:03<00:39, 22.92it/s]\u001b[A\n",
      " 10%|█         | 101/1000 [00:03<00:39, 22.63it/s]\u001b[A\n",
      " 10%|█         | 104/1000 [00:03<00:39, 22.57it/s]\u001b[A\n",
      " 11%|█         | 107/1000 [00:03<00:39, 22.78it/s]\u001b[A\n",
      " 11%|█         | 110/1000 [00:03<00:38, 22.98it/s]\u001b[A\n",
      " 11%|█▏        | 113/1000 [00:04<00:38, 22.83it/s]\u001b[A\n",
      " 12%|█▏        | 116/1000 [00:04<00:38, 22.78it/s]\u001b[A\n",
      " 12%|█▏        | 119/1000 [00:04<00:39, 22.42it/s]\u001b[A\n",
      " 12%|█▏        | 122/1000 [00:04<00:39, 22.38it/s]\u001b[A\n",
      " 12%|█▎        | 125/1000 [00:04<00:38, 22.58it/s]\u001b[A\n",
      " 13%|█▎        | 128/1000 [00:04<00:38, 22.87it/s]\u001b[A\n",
      " 13%|█▎        | 131/1000 [00:04<00:38, 22.79it/s]\u001b[A\n",
      " 13%|█▎        | 134/1000 [00:05<00:40, 21.63it/s]\u001b[A\n",
      " 14%|█▎        | 137/1000 [00:05<00:39, 21.79it/s]\u001b[A\n",
      " 14%|█▍        | 140/1000 [00:05<00:39, 21.54it/s]\u001b[A\n",
      " 14%|█▍        | 143/1000 [00:05<00:39, 21.73it/s]\u001b[A\n",
      " 15%|█▍        | 146/1000 [00:05<00:38, 22.33it/s]\u001b[A\n",
      " 15%|█▍        | 149/1000 [00:05<00:38, 22.35it/s]\u001b[A\n",
      " 15%|█▌        | 152/1000 [00:05<00:37, 22.85it/s]\u001b[A\n",
      " 16%|█▌        | 155/1000 [00:05<00:36, 22.88it/s]\u001b[A\n",
      " 16%|█▌        | 158/1000 [00:06<00:37, 22.70it/s]\u001b[A\n",
      " 16%|█▌        | 161/1000 [00:06<00:35, 23.37it/s]\u001b[A\n",
      " 16%|█▋        | 164/1000 [00:06<00:35, 23.30it/s]\u001b[A\n",
      " 17%|█▋        | 167/1000 [00:06<00:35, 23.33it/s]\u001b[A\n",
      " 17%|█▋        | 170/1000 [00:06<00:35, 23.65it/s]\u001b[A\n",
      " 17%|█▋        | 173/1000 [00:06<00:34, 23.98it/s]\u001b[A\n",
      " 18%|█▊        | 176/1000 [00:06<00:35, 23.52it/s]\u001b[A\n",
      " 18%|█▊        | 179/1000 [00:07<00:34, 23.75it/s]\u001b[A\n",
      " 18%|█▊        | 182/1000 [00:07<00:35, 23.06it/s]\u001b[A\n",
      " 18%|█▊        | 185/1000 [00:07<00:35, 23.03it/s]\u001b[A\n",
      " 19%|█▉        | 188/1000 [00:07<00:36, 22.37it/s]\u001b[A\n",
      " 19%|█▉        | 191/1000 [00:07<00:35, 22.57it/s]\u001b[A\n",
      " 19%|█▉        | 194/1000 [00:07<00:35, 22.67it/s]\u001b[A\n",
      " 20%|█▉        | 197/1000 [00:07<00:35, 22.68it/s]\u001b[A\n",
      " 20%|██        | 200/1000 [00:07<00:35, 22.62it/s]\u001b[A\n",
      " 20%|██        | 203/1000 [00:08<00:36, 22.11it/s]\u001b[A\n",
      " 21%|██        | 206/1000 [00:08<00:35, 22.21it/s]\u001b[A\n",
      " 21%|██        | 209/1000 [00:08<00:36, 21.72it/s]\u001b[A\n",
      " 21%|██        | 212/1000 [00:08<00:36, 21.74it/s]\u001b[A\n",
      " 22%|██▏       | 215/1000 [00:08<00:35, 21.92it/s]\u001b[A\n",
      " 22%|██▏       | 218/1000 [00:08<00:35, 22.13it/s]\u001b[A\n",
      " 22%|██▏       | 221/1000 [00:08<00:34, 22.56it/s]\u001b[A\n",
      " 22%|██▏       | 224/1000 [00:09<00:35, 21.98it/s]\u001b[A\n",
      " 23%|██▎       | 227/1000 [00:09<00:35, 21.83it/s]\u001b[A\n",
      " 23%|██▎       | 230/1000 [00:09<00:36, 21.18it/s]\u001b[A\n",
      " 23%|██▎       | 233/1000 [00:09<00:37, 20.72it/s]\u001b[A\n",
      " 24%|██▎       | 236/1000 [00:09<00:36, 20.67it/s]\u001b[A\n",
      " 24%|██▍       | 239/1000 [00:09<00:35, 21.31it/s]\u001b[A\n",
      " 24%|██▍       | 242/1000 [00:09<00:34, 21.82it/s]\u001b[A\n",
      " 24%|██▍       | 245/1000 [00:10<00:34, 21.60it/s]\u001b[A\n",
      " 25%|██▍       | 248/1000 [00:10<00:34, 21.72it/s]\u001b[A\n",
      " 25%|██▌       | 251/1000 [00:10<00:33, 22.09it/s]\u001b[A\n",
      " 25%|██▌       | 254/1000 [00:10<00:33, 22.44it/s]\u001b[A\n",
      " 26%|██▌       | 257/1000 [00:10<00:32, 22.93it/s]\u001b[A\n",
      " 26%|██▌       | 260/1000 [00:10<00:33, 22.19it/s]\u001b[A\n",
      " 26%|██▋       | 263/1000 [00:10<00:33, 22.23it/s]\u001b[A\n",
      " 27%|██▋       | 266/1000 [00:10<00:33, 22.02it/s]\u001b[A\n",
      " 27%|██▋       | 269/1000 [00:11<00:33, 21.74it/s]\u001b[A\n",
      " 27%|██▋       | 272/1000 [00:11<00:33, 21.85it/s]\u001b[A\n",
      " 28%|██▊       | 275/1000 [00:11<00:33, 21.69it/s]\u001b[A\n",
      " 28%|██▊       | 278/1000 [00:11<00:33, 21.68it/s]\u001b[A\n",
      " 28%|██▊       | 281/1000 [00:11<00:32, 22.28it/s]\u001b[A\n",
      " 28%|██▊       | 284/1000 [00:11<00:31, 22.91it/s]\u001b[A\n",
      " 29%|██▊       | 287/1000 [00:11<00:31, 22.81it/s]\u001b[A\n",
      " 29%|██▉       | 290/1000 [00:12<00:31, 22.79it/s]\u001b[A\n",
      " 29%|██▉       | 293/1000 [00:12<00:31, 22.46it/s]\u001b[A\n",
      " 30%|██▉       | 296/1000 [00:12<00:31, 22.49it/s]\u001b[A\n",
      " 30%|██▉       | 299/1000 [00:12<00:31, 22.54it/s]\u001b[A\n",
      " 30%|███       | 302/1000 [00:12<00:30, 23.24it/s]\u001b[A\n",
      " 30%|███       | 305/1000 [00:12<00:29, 23.42it/s]\u001b[A\n",
      " 31%|███       | 308/1000 [00:12<00:30, 23.03it/s]\u001b[A\n",
      " 31%|███       | 311/1000 [00:12<00:29, 23.09it/s]\u001b[A\n",
      " 31%|███▏      | 314/1000 [00:13<00:29, 23.08it/s]\u001b[A\n",
      " 32%|███▏      | 317/1000 [00:13<00:29, 22.79it/s]\u001b[A\n",
      " 32%|███▏      | 320/1000 [00:13<00:30, 22.63it/s]\u001b[A\n",
      " 32%|███▏      | 323/1000 [00:13<00:30, 22.26it/s]\u001b[A\n",
      " 33%|███▎      | 326/1000 [00:13<00:30, 22.10it/s]\u001b[A\n",
      " 33%|███▎      | 329/1000 [00:13<00:30, 22.30it/s]\u001b[A\n",
      " 33%|███▎      | 332/1000 [00:13<00:30, 22.18it/s]\u001b[A\n",
      " 34%|███▎      | 335/1000 [00:14<00:29, 22.35it/s]\u001b[A\n",
      " 34%|███▍      | 338/1000 [00:14<00:30, 21.95it/s]\u001b[A\n",
      " 34%|███▍      | 341/1000 [00:14<00:29, 22.02it/s]\u001b[A\n",
      " 34%|███▍      | 344/1000 [00:14<00:29, 22.13it/s]\u001b[A\n",
      " 35%|███▍      | 347/1000 [00:14<00:28, 22.77it/s]\u001b[A\n",
      " 35%|███▌      | 350/1000 [00:14<00:29, 22.31it/s]\u001b[A\n",
      " 35%|███▌      | 353/1000 [00:14<00:28, 22.33it/s]\u001b[A\n",
      " 36%|███▌      | 356/1000 [00:14<00:29, 21.91it/s]\u001b[A\n",
      " 36%|███▌      | 359/1000 [00:15<00:29, 21.60it/s]\u001b[A\n",
      " 36%|███▋      | 363/1000 [00:15<00:26, 24.49it/s]\u001b[A\n",
      " 37%|███▋      | 366/1000 [00:15<00:24, 25.66it/s]\u001b[A\n",
      " 37%|███▋      | 370/1000 [00:15<00:22, 27.81it/s]\u001b[A\n",
      " 37%|███▋      | 374/1000 [00:15<00:21, 29.51it/s]\u001b[A\n",
      " 38%|███▊      | 378/1000 [00:15<00:20, 29.89it/s]\u001b[A\n",
      " 38%|███▊      | 382/1000 [00:15<00:19, 31.06it/s]\u001b[A\n",
      " 39%|███▊      | 386/1000 [00:15<00:19, 30.82it/s]\u001b[A\n",
      " 39%|███▉      | 390/1000 [00:16<00:19, 31.68it/s]\u001b[A\n",
      " 39%|███▉      | 394/1000 [00:16<00:19, 31.57it/s]\u001b[A\n",
      " 40%|███▉      | 398/1000 [00:16<00:18, 32.27it/s]\u001b[A\n",
      " 40%|████      | 402/1000 [00:16<00:18, 32.90it/s]\u001b[A\n",
      " 41%|████      | 406/1000 [00:16<00:18, 32.99it/s]\u001b[A\n",
      " 41%|████      | 410/1000 [00:16<00:18, 31.97it/s]\u001b[A\n",
      " 41%|████▏     | 414/1000 [00:16<00:18, 32.41it/s]\u001b[A\n",
      " 42%|████▏     | 418/1000 [00:16<00:17, 32.76it/s]\u001b[A\n",
      " 42%|████▏     | 422/1000 [00:17<00:17, 32.53it/s]\u001b[A\n",
      " 43%|████▎     | 426/1000 [00:17<00:18, 31.67it/s]\u001b[A\n",
      " 43%|████▎     | 430/1000 [00:17<00:17, 32.32it/s]\u001b[A\n",
      " 43%|████▎     | 434/1000 [00:17<00:17, 32.73it/s]\u001b[A\n",
      " 44%|████▍     | 438/1000 [00:17<00:17, 32.74it/s]\u001b[A\n",
      " 44%|████▍     | 442/1000 [00:17<00:16, 32.93it/s]\u001b[A\n",
      " 45%|████▍     | 446/1000 [00:17<00:16, 33.51it/s]\u001b[A\n",
      " 45%|████▌     | 450/1000 [00:17<00:16, 33.37it/s]\u001b[A\n",
      " 45%|████▌     | 454/1000 [00:18<00:16, 33.22it/s]\u001b[A\n",
      " 46%|████▌     | 458/1000 [00:18<00:16, 32.66it/s]\u001b[A\n",
      " 46%|████▌     | 462/1000 [00:18<00:16, 32.83it/s]\u001b[A\n",
      " 47%|████▋     | 466/1000 [00:18<00:16, 32.97it/s]\u001b[A\n",
      " 47%|████▋     | 470/1000 [00:18<00:16, 32.36it/s]\u001b[A\n",
      " 47%|████▋     | 474/1000 [00:18<00:16, 32.47it/s]\u001b[A\n",
      " 48%|████▊     | 478/1000 [00:18<00:16, 31.96it/s]\u001b[A\n",
      " 48%|████▊     | 482/1000 [00:18<00:15, 32.67it/s]\u001b[A\n",
      " 49%|████▊     | 486/1000 [00:19<00:15, 32.62it/s]\u001b[A\n",
      " 49%|████▉     | 490/1000 [00:19<00:15, 32.63it/s]\u001b[A\n",
      " 49%|████▉     | 494/1000 [00:19<00:15, 32.90it/s]\u001b[A\n",
      " 50%|████▉     | 498/1000 [00:19<00:15, 33.15it/s]\u001b[A\n",
      " 50%|█████     | 502/1000 [00:19<00:14, 33.28it/s]\u001b[A\n",
      " 51%|█████     | 506/1000 [00:19<00:15, 31.54it/s]\u001b[A\n",
      " 51%|█████     | 510/1000 [00:19<00:15, 31.95it/s]\u001b[A\n",
      " 51%|█████▏    | 514/1000 [00:19<00:15, 32.29it/s]\u001b[A\n",
      " 52%|█████▏    | 518/1000 [00:19<00:14, 32.81it/s]\u001b[A\n",
      " 52%|█████▏    | 522/1000 [00:20<00:14, 32.92it/s]\u001b[A\n",
      " 53%|█████▎    | 526/1000 [00:20<00:14, 32.90it/s]\u001b[A\n",
      " 53%|█████▎    | 530/1000 [00:20<00:14, 33.34it/s]\u001b[A\n",
      " 53%|█████▎    | 534/1000 [00:20<00:13, 33.55it/s]\u001b[A\n",
      " 54%|█████▍    | 538/1000 [00:20<00:13, 33.44it/s]\u001b[A\n",
      " 54%|█████▍    | 542/1000 [00:20<00:13, 32.85it/s]\u001b[A\n",
      " 55%|█████▍    | 546/1000 [00:20<00:13, 33.24it/s]\u001b[A\n",
      " 55%|█████▌    | 550/1000 [00:20<00:13, 33.45it/s]\u001b[A\n",
      " 55%|█████▌    | 554/1000 [00:21<00:13, 33.50it/s]\u001b[A\n",
      " 56%|█████▌    | 558/1000 [00:21<00:13, 33.73it/s]\u001b[A\n",
      " 56%|█████▌    | 562/1000 [00:21<00:12, 33.87it/s]\u001b[A\n",
      " 57%|█████▋    | 566/1000 [00:21<00:12, 34.06it/s]\u001b[A\n",
      " 57%|█████▋    | 570/1000 [00:21<00:12, 34.02it/s]\u001b[A\n",
      " 57%|█████▋    | 574/1000 [00:21<00:12, 33.99it/s]\u001b[A\n",
      " 58%|█████▊    | 578/1000 [00:21<00:12, 33.87it/s]\u001b[A\n",
      " 58%|█████▊    | 582/1000 [00:21<00:12, 33.85it/s]\u001b[A\n",
      " 59%|█████▊    | 586/1000 [00:22<00:12, 33.65it/s]\u001b[A\n",
      " 59%|█████▉    | 590/1000 [00:22<00:12, 33.15it/s]\u001b[A\n",
      " 59%|█████▉    | 594/1000 [00:22<00:12, 33.26it/s]\u001b[A\n",
      " 60%|█████▉    | 598/1000 [00:22<00:11, 33.57it/s]\u001b[A\n",
      " 60%|██████    | 602/1000 [00:22<00:11, 33.74it/s]\u001b[A\n",
      " 61%|██████    | 606/1000 [00:22<00:11, 33.73it/s]\u001b[A\n",
      " 61%|██████    | 610/1000 [00:22<00:11, 33.59it/s]\u001b[A\n",
      " 61%|██████▏   | 614/1000 [00:22<00:11, 33.38it/s]\u001b[A\n",
      " 62%|██████▏   | 618/1000 [00:22<00:11, 33.63it/s]\u001b[A\n",
      " 62%|██████▏   | 622/1000 [00:23<00:11, 33.56it/s]\u001b[A\n",
      " 63%|██████▎   | 626/1000 [00:23<00:11, 33.44it/s]\u001b[A\n",
      " 63%|██████▎   | 630/1000 [00:23<00:10, 33.66it/s]\u001b[A\n",
      " 63%|██████▎   | 634/1000 [00:23<00:11, 32.14it/s]\u001b[A\n",
      " 64%|██████▍   | 638/1000 [00:23<00:11, 31.89it/s]\u001b[A\n",
      " 64%|██████▍   | 642/1000 [00:23<00:12, 28.14it/s]\u001b[A\n",
      " 64%|██████▍   | 645/1000 [00:23<00:13, 26.11it/s]\u001b[A\n",
      " 65%|██████▍   | 648/1000 [00:24<00:14, 24.56it/s]\u001b[A\n",
      " 65%|██████▌   | 651/1000 [00:24<00:14, 23.40it/s]\u001b[A\n",
      " 65%|██████▌   | 654/1000 [00:24<00:14, 23.07it/s]\u001b[A\n",
      " 66%|██████▌   | 657/1000 [00:24<00:15, 22.49it/s]\u001b[A\n",
      " 66%|██████▌   | 660/1000 [00:24<00:15, 22.45it/s]\u001b[A\n",
      " 66%|██████▋   | 663/1000 [00:24<00:15, 22.21it/s]\u001b[A\n",
      " 67%|██████▋   | 666/1000 [00:24<00:15, 22.09it/s]\u001b[A\n",
      " 67%|██████▋   | 669/1000 [00:25<00:15, 21.41it/s]\u001b[A\n",
      " 67%|██████▋   | 672/1000 [00:25<00:15, 21.27it/s]\u001b[A\n",
      " 68%|██████▊   | 675/1000 [00:25<00:15, 21.55it/s]\u001b[A\n",
      " 68%|██████▊   | 678/1000 [00:25<00:14, 21.70it/s]\u001b[A\n",
      " 68%|██████▊   | 681/1000 [00:25<00:14, 21.86it/s]\u001b[A\n",
      " 68%|██████▊   | 684/1000 [00:25<00:14, 21.50it/s]\u001b[A\n",
      " 69%|██████▊   | 687/1000 [00:25<00:15, 20.63it/s]\u001b[A\n",
      " 69%|██████▉   | 690/1000 [00:26<00:15, 20.58it/s]\u001b[A\n",
      " 69%|██████▉   | 693/1000 [00:26<00:15, 20.17it/s]\u001b[A\n",
      " 70%|██████▉   | 696/1000 [00:26<00:14, 20.49it/s]\u001b[A\n",
      " 70%|██████▉   | 699/1000 [00:26<00:14, 20.74it/s]\u001b[A\n",
      " 70%|███████   | 702/1000 [00:26<00:14, 21.13it/s]\u001b[A\n",
      " 70%|███████   | 705/1000 [00:26<00:14, 20.93it/s]\u001b[A\n",
      " 71%|███████   | 708/1000 [00:26<00:13, 22.14it/s]\u001b[A\n",
      " 71%|███████   | 711/1000 [00:27<00:13, 20.77it/s]\u001b[A\n",
      " 71%|███████▏  | 714/1000 [00:27<00:14, 19.67it/s]\u001b[A\n",
      " 72%|███████▏  | 717/1000 [00:27<00:14, 20.06it/s]\u001b[A\n",
      " 72%|███████▏  | 720/1000 [00:27<00:12, 21.56it/s]\u001b[A\n",
      " 72%|███████▏  | 723/1000 [00:27<00:11, 23.10it/s]\u001b[A\n",
      " 73%|███████▎  | 726/1000 [00:27<00:11, 23.22it/s]\u001b[A\n",
      " 73%|███████▎  | 729/1000 [00:27<00:11, 23.44it/s]\u001b[A\n",
      " 73%|███████▎  | 732/1000 [00:27<00:11, 23.86it/s]\u001b[A\n",
      " 74%|███████▎  | 735/1000 [00:28<00:10, 24.37it/s]\u001b[A\n",
      " 74%|███████▍  | 738/1000 [00:28<00:10, 24.45it/s]\u001b[A\n",
      " 74%|███████▍  | 741/1000 [00:28<00:10, 24.32it/s]\u001b[A\n",
      " 74%|███████▍  | 744/1000 [00:28<00:10, 24.46it/s]\u001b[A\n",
      " 75%|███████▍  | 747/1000 [00:28<00:10, 24.49it/s]\u001b[A\n",
      " 75%|███████▌  | 750/1000 [00:28<00:10, 24.07it/s]\u001b[A\n",
      " 75%|███████▌  | 753/1000 [00:28<00:10, 24.61it/s]\u001b[A\n",
      " 76%|███████▌  | 756/1000 [00:28<00:09, 25.61it/s]\u001b[A\n",
      " 76%|███████▌  | 759/1000 [00:29<00:09, 24.99it/s]\u001b[A\n",
      " 76%|███████▌  | 762/1000 [00:29<00:09, 25.37it/s]\u001b[A\n",
      " 76%|███████▋  | 765/1000 [00:29<00:09, 25.83it/s]\u001b[A\n",
      " 77%|███████▋  | 768/1000 [00:29<00:08, 26.38it/s]\u001b[A\n",
      " 77%|███████▋  | 771/1000 [00:29<00:08, 26.19it/s]\u001b[A\n",
      " 77%|███████▋  | 774/1000 [00:29<00:08, 25.93it/s]\u001b[A\n",
      " 78%|███████▊  | 777/1000 [00:29<00:08, 25.81it/s]\u001b[A\n",
      " 78%|███████▊  | 781/1000 [00:29<00:07, 27.43it/s]\u001b[A\n",
      " 78%|███████▊  | 785/1000 [00:29<00:07, 28.79it/s]\u001b[A\n",
      " 79%|███████▉  | 788/1000 [00:30<00:07, 28.50it/s]\u001b[A\n",
      " 79%|███████▉  | 791/1000 [00:30<00:07, 28.57it/s]\u001b[A\n",
      " 80%|███████▉  | 795/1000 [00:30<00:06, 29.68it/s]\u001b[A\n",
      " 80%|███████▉  | 798/1000 [00:30<00:06, 29.66it/s]\u001b[A\n",
      " 80%|████████  | 801/1000 [00:30<00:06, 29.44it/s]\u001b[A\n",
      " 80%|████████  | 804/1000 [00:30<00:06, 29.27it/s]\u001b[A\n",
      " 81%|████████  | 808/1000 [00:30<00:06, 30.21it/s]\u001b[A\n",
      " 81%|████████  | 812/1000 [00:30<00:06, 29.72it/s]\u001b[A\n",
      " 82%|████████▏ | 815/1000 [00:30<00:06, 27.42it/s]\u001b[A\n",
      " 82%|████████▏ | 818/1000 [00:31<00:07, 24.84it/s]\u001b[A\n",
      " 82%|████████▏ | 821/1000 [00:31<00:07, 24.69it/s]\u001b[A\n",
      " 82%|████████▏ | 824/1000 [00:31<00:07, 24.73it/s]\u001b[A\n",
      " 83%|████████▎ | 828/1000 [00:31<00:06, 26.67it/s]\u001b[A\n",
      " 83%|████████▎ | 832/1000 [00:31<00:05, 28.46it/s]\u001b[A\n",
      " 84%|████████▎ | 835/1000 [00:31<00:05, 28.42it/s]\u001b[A\n",
      " 84%|████████▍ | 839/1000 [00:31<00:05, 29.52it/s]\u001b[A\n",
      " 84%|████████▍ | 843/1000 [00:31<00:05, 30.50it/s]\u001b[A\n",
      " 85%|████████▍ | 847/1000 [00:32<00:05, 30.09it/s]\u001b[A\n",
      " 85%|████████▌ | 851/1000 [00:32<00:04, 30.21it/s]\u001b[A\n",
      " 86%|████████▌ | 855/1000 [00:32<00:04, 30.45it/s]\u001b[A\n",
      " 86%|████████▌ | 859/1000 [00:32<00:04, 31.24it/s]\u001b[A\n",
      " 86%|████████▋ | 863/1000 [00:32<00:04, 31.94it/s]\u001b[A\n",
      " 87%|████████▋ | 867/1000 [00:32<00:04, 32.51it/s]\u001b[A\n",
      " 87%|████████▋ | 871/1000 [00:32<00:04, 31.27it/s]\u001b[A\n",
      " 88%|████████▊ | 875/1000 [00:32<00:03, 31.75it/s]\u001b[A\n",
      " 88%|████████▊ | 879/1000 [00:33<00:03, 31.87it/s]\u001b[A\n",
      " 88%|████████▊ | 883/1000 [00:33<00:03, 32.45it/s]\u001b[A\n",
      " 89%|████████▊ | 887/1000 [00:33<00:03, 31.15it/s]\u001b[A\n",
      " 89%|████████▉ | 891/1000 [00:33<00:03, 30.35it/s]\u001b[A\n",
      " 90%|████████▉ | 895/1000 [00:33<00:03, 28.23it/s]\u001b[A\n",
      " 90%|████████▉ | 898/1000 [00:33<00:03, 26.07it/s]\u001b[A\n",
      " 90%|█████████ | 901/1000 [00:33<00:04, 24.57it/s]\u001b[A\n",
      " 90%|█████████ | 904/1000 [00:34<00:04, 22.76it/s]\u001b[A\n",
      " 91%|█████████ | 907/1000 [00:34<00:04, 22.69it/s]\u001b[A\n",
      " 91%|█████████ | 910/1000 [00:34<00:03, 22.54it/s]\u001b[A\n",
      " 91%|█████████▏| 913/1000 [00:34<00:03, 22.58it/s]\u001b[A\n",
      " 92%|█████████▏| 916/1000 [00:34<00:03, 21.69it/s]\u001b[A\n",
      " 92%|█████████▏| 919/1000 [00:34<00:03, 21.65it/s]\u001b[A\n",
      " 92%|█████████▏| 922/1000 [00:34<00:03, 21.60it/s]\u001b[A\n",
      " 92%|█████████▎| 925/1000 [00:35<00:03, 20.97it/s]\u001b[A\n",
      " 93%|█████████▎| 928/1000 [00:35<00:03, 21.29it/s]\u001b[A\n",
      " 93%|█████████▎| 931/1000 [00:35<00:03, 21.44it/s]\u001b[A\n",
      " 93%|█████████▎| 934/1000 [00:35<00:03, 18.99it/s]\u001b[A\n",
      " 94%|█████████▎| 937/1000 [00:35<00:03, 20.05it/s]\u001b[A\n",
      " 94%|█████████▍| 940/1000 [00:35<00:02, 20.04it/s]\u001b[A\n",
      " 94%|█████████▍| 943/1000 [00:36<00:03, 18.19it/s]\u001b[A\n",
      " 95%|█████████▍| 946/1000 [00:36<00:02, 19.81it/s]\u001b[A\n",
      " 95%|█████████▍| 949/1000 [00:36<00:02, 20.09it/s]\u001b[A\n",
      " 95%|█████████▌| 952/1000 [00:36<00:02, 19.84it/s]\u001b[A\n",
      " 96%|█████████▌| 955/1000 [00:36<00:02, 20.21it/s]\u001b[A\n",
      " 96%|█████████▌| 958/1000 [00:36<00:02, 20.56it/s]\u001b[A\n",
      " 96%|█████████▌| 961/1000 [00:36<00:01, 20.98it/s]\u001b[A\n",
      " 96%|█████████▋| 964/1000 [00:37<00:01, 21.36it/s]\u001b[A\n",
      " 97%|█████████▋| 967/1000 [00:37<00:01, 22.12it/s]\u001b[A\n",
      " 97%|█████████▋| 970/1000 [00:37<00:01, 21.70it/s]\u001b[A\n",
      " 97%|█████████▋| 973/1000 [00:37<00:01, 21.41it/s]\u001b[A\n",
      " 98%|█████████▊| 976/1000 [00:37<00:01, 21.28it/s]\u001b[A\n",
      " 98%|█████████▊| 979/1000 [00:37<00:00, 21.25it/s]\u001b[A\n",
      " 98%|█████████▊| 982/1000 [00:37<00:00, 21.34it/s]\u001b[A\n",
      " 98%|█████████▊| 985/1000 [00:37<00:00, 21.40it/s]\u001b[A\n",
      " 99%|█████████▉| 988/1000 [00:38<00:00, 22.39it/s]\u001b[A\n",
      " 99%|█████████▉| 992/1000 [00:38<00:00, 24.65it/s]\u001b[A\n",
      "100%|█████████▉| 996/1000 [00:38<00:00, 26.56it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:38<00:00, 25.98it/s][A\n",
      "100%|██████████| 1/1 [00:40<00:00, 40.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1000 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Test Sarsa Agent \n",
    "num_runs = 1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Environment\n",
    "env_to_wrap = gym.make('Pendulum-v0')\n",
    "# Maximum number of possible iterations (default was 200)\n",
    "env_to_wrap._max_episode_steps = 1500\n",
    "env = Monitor(env_to_wrap, \"./videos/pendulum\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "for n_runs in tqdm(range(num_runs)):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "        \n",
    "    # Times the environment will start again without resetting the agent\n",
    "    for t in tqdm(range(num_episodes)):\n",
    "\n",
    "        # View environment\n",
    "        env.render()\n",
    "\n",
    "        # Take a step with the environment\n",
    "        observation, reward, done, info = env.step(last_action)\n",
    "\n",
    "        # If the goal has been reached stop\n",
    "        if done:\n",
    "            # Last step with the agent\n",
    "            agent.agent_end(reward)\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Take a step with the agent\n",
    "            last_action = agent.agent_step(reward, observation)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env_to_wrap.close()\n",
    "\n",
    "print(\"Episode finished after {} timesteps\".format(t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
