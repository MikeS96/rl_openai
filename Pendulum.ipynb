{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Pendulum with function appoximation and control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is intended to solve the Episodic Mountain car problem using Semi-gradient sarsa and Tile Coding.\n",
    "\n",
    "The description of the problem is given below:\n",
    "\n",
    "\"The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright..\" \n",
    "\n",
    "<img src=\"./assets/car.png\" width=\"380\" />\n",
    "\n",
    "An extensive description and solution of the problem can be seen here [Section 10.1 Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=267)\n",
    "\n",
    "Image and Text taken from Taken from [Official documentaiton Mountain car](https://gym.openai.com/envs/Pendulum-v0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undestanding the Workflow of OpenAI\n",
    "\n",
    "The following variables are used at each timestep and they are returned by the Mountain Car environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **observation** (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "- **reward** (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "- **done** (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "- **info** (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick recap, the diagram below explains the workflow of a Markov Decision Process (MDP)\n",
    "\n",
    "<img src=\"./assets/MDP.png\" width=\"380\" />\n",
    "\n",
    "Image taken from [Section 3.1 Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Agent specifications\n",
    "\n",
    "Below are presented the main features of the environment and agent. Overall, the action space of the problem is discrete with three posible actions. The observations or state space is continuios, therefore it is necessary to use a function approximation technique to solve this challenge. The agent receives a reward of -1 at each timestep unless it reaches the goal. The episode ends if the agent reaches the goal or a specific number of iterations are done. Additionally, the agent will always start at a random position between $-0.6$ and $-0.4$ with zero velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "\n",
    "     Type:  Box(2)\n",
    "     Num \tObservation \t Min \tMax\n",
    "     0 \t  cos(theta)      -1.0 \t1.0\n",
    "     1 \t  sin(theta) \t -1.0 \t1.0\n",
    "     2       theta dot \t  -8.0 \t8.0\n",
    "         \n",
    "**Actions**:\n",
    "\n",
    "     Type: Box(1)\n",
    "     Num \tAction \t        Min \tMax\n",
    "     0 \t  Joint effort      -2.0 \t2.0\n",
    "\n",
    "        \n",
    "**Reward**:\n",
    "\n",
    "     -(theta^2 + 0.1*theta_dt^2 + 0.001*action^2)\n",
    "\n",
    "        \n",
    "**Starting State**:\n",
    "\n",
    "     Random angle from -pi to pi, and random velocity between -1 and 1\n",
    "        \n",
    "**Episode Termination**:\n",
    "\n",
    "     Continuous problem\n",
    "     \n",
    "For further information see [Github source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell aims to show how to iterate with the action and observation space of the agent and extract relevant information from it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Action Space is an object of type: Box(1,)\n",
      "\n",
      "The shape of the action space is: (1,)\n",
      "\n",
      "The High values in the action space are [2.], the low values are [-2.]\n",
      "\n",
      "The Environment Space is an object of type: Box(3,)\n",
      "\n",
      "The Shape of the dimension Space are: (3,)\n",
      "\n",
      "The High values in the observation space are [1. 1. 8.], the low values are [-1. -1. -8.]\n",
      "\n",
      "The Observations at a given timestep are [ 0.02874173 -0.3922771  -6.9678636 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "observation = env.reset() \n",
    "\n",
    "# Object's type in the action Space\n",
    "print(\"The Action Space is an object of type: {0}\\n\".format(env.action_space))\n",
    "# Shape of the action Space\n",
    "print(\"The shape of the action space is: {0}\\n\".format(env.action_space.shape))\n",
    "# The high and low values in the action space\n",
    "print(\"The High values in the action space are {0}, the low values are {1}\\n\".format(\n",
    "    env.action_space.high, env.action_space.low))\n",
    "# Object's type in the Observation Space\n",
    "print(\"The Environment Space is an object of type: {0}\\n\".format(env.observation_space))\n",
    "# Shape of the observation space\n",
    "print(\"The Shape of the dimension Space are: {0}\\n\".format(env.observation_space.shape))\n",
    "# The high and low values in the observation space\n",
    "print(\"The High values in the observation space are {0}, the low values are {1}\\n\".format(\n",
    "    env.observation_space.high, env.observation_space.low))\n",
    "# Example of observation\n",
    "print(\"The Observations at a given timestep are {0}\\n\".format(env.observation_space.sample()))\n",
    "\n",
    "# https://medium.com/deeplearningmadeeasy/advantage-actor-critic-continuous-case-implementation-f55ce5da6b4c\n",
    "# https://www.coursera.org/learn/prediction-control-function-approximation/home/welcome\n",
    "# [Section 3.1 Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=357)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile Coding Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete explanation about what is tile coding and how it works, see [Section 9.5.4 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=239). Overall, this is a way to create features that can both provide good generalization and discrimination for value function approximation. Tile coding consists of multiple overlapping tiling, where each tiling is a partitioning of the space into tiles.\n",
    "\n",
    "<img src=\"./assets/tilecoding.png\" width=\"640\" />\n",
    "\n",
    "**Note**: Tile coding can be only be used with 2d observation spaces.\n",
    "\n",
    "This technique is implemented using Tiles3, which is a python library written by Richard S. Sutton. For the full documentation see [Tiles3 documentation](http://incompleteideas.net/tiles/tiles3.html)\n",
    "\n",
    "Image taken from [Section 9.5.4 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Neural Network\n",
    "class Critic(nn.Module):\n",
    "    # Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).\n",
    "    def __init__(self, critic_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # Number of states\n",
    "        self.state_dim = critic_config.get(\"state_dim\")\n",
    "        # Hidden units\n",
    "        self.num_hidden_units = critic_config.get(\"num_hidden_units\")\n",
    "        \n",
    "        a1 = int(self.num_hidden_units / 4)\n",
    "        a2 = int(self.num_hidden_units / 2)\n",
    "        \n",
    "        # Initialzie first hidden layer \n",
    "        self.hidden_1 = nn.Linear(self.state_dim, a1)\n",
    "        # Initialzie second hidden layer \n",
    "        self.hidden_2 = nn.Linear(a1, a2)\n",
    "        # Initialzie Third hidden layer \n",
    "        self.hidden_3 = nn.Linear(a2, self.num_hidden_units)\n",
    "        # Initialize output layer\n",
    "        self.output = nn.Linear(self.num_hidden_units, 1)\n",
    "                        \n",
    "    \n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This is a feed-forward pass in the network\n",
    "        Args:\n",
    "            s (Numpy array): The state, a 2D array of shape (batch_size, state_dim)\n",
    "        Returns:\n",
    "            The action-values (Torch array) calculated using the network's weights.\n",
    "            A 2D array of shape (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # Transform observations into a pytorch tensor\n",
    "        s = torch.Tensor(s)\n",
    "        \n",
    "        v = F.relu(self.hidden_1(s))\n",
    "        v = F.relu(self.hidden_2(v))\n",
    "        v = F.relu(self.hidden_3(v))\n",
    "        v = self.output(v)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actor neural network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,  actor_config):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Number of states\n",
    "        self.state_dim = actor_config.get(\"state_dim\")\n",
    "        # Hidden units\n",
    "        self.num_hidden_units = actor_config.get(\"num_hidden_units\")\n",
    "        # Actions or output units\n",
    "        self.num_actions = actor_config.get(\"num_actions\")\n",
    "        \n",
    "        a1 = int(self.num_hidden_units / 4)\n",
    "        a2 = int(self.num_hidden_units / 2)\n",
    "        \n",
    "        # Initialzie first hidden layer \n",
    "        self.hidden_1 = nn.Linear(self.state_dim, a1)\n",
    "        # Initialzie second hidden layer \n",
    "        self.hidden_2 = nn.Linear(a1, a2)\n",
    "        # Initialzie Third hidden layer \n",
    "        self.hidden_3 = nn.Linear(a2, self.num_hidden_units)\n",
    "        # Initialize output layer\n",
    "        self.output = nn.Linear(self.num_hidden_units, self.num_actions * 2)\n",
    "        \n",
    "        # Log of standard deviation\n",
    "        # logstdv_param = nn.Parameter(torch.full((self.num_actions,), 0.1))\n",
    "        # Register parameter in the network\n",
    "        # self.register_parameter(\"logstdv\", logstdv_param)\n",
    "                \n",
    "    def compute_mean(self, s):\n",
    "        \"\"\"\n",
    "        This is a feed-forward pass in the network\n",
    "        Args:\n",
    "            s (Numpy array): The state, a 2D array of shape (batch_size, state_dim)\n",
    "        Returns:\n",
    "            The action-values (Torch array) calculated using the network's weights.\n",
    "            A 2D array of shape (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # Transform observations into a pytorch tensor\n",
    "        s = torch.Tensor(s)\n",
    "        \n",
    "        pi = F.relu(self.hidden_1(s))\n",
    "        pi = F.relu(self.hidden_2(pi))\n",
    "        pi = F.relu(self.hidden_3(pi))\n",
    "        pi = self.output(pi)\n",
    "        \n",
    "        return pi\n",
    "                \n",
    "    \n",
    "    def forward(self, s):\n",
    "        \n",
    "        # Compute the mean with the model\n",
    "        mean, logstdv = self.compute_mean(s)\n",
    "        # Clamp the stdv between 1e-3 and 50\n",
    "        #stdv = torch.clamp(logstdv.exp(), 1e-3, 50)\n",
    "        stdv = logstdv.exp()\n",
    "        \n",
    "        # Sample an action from the normal distribution\n",
    "        return torch.distributions.Normal(mean, stdv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TD agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute the TD Target and TD estimate\n",
    "def get_td(states, next_states, rewards, terminals, actor, critic, avg_reward):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor (gamma).\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        target_vec (Tensor array): The TD Target for actions taken, of shape (batch_size,)\n",
    "        estimate_vec (Tensor array): The TD estimate for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used \n",
    "    # for computing the  targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # q_next_mat is a 2D Tensor of shape (batch_size, num_actions)\n",
    "    # used to compute the action-values of the next states\n",
    "    # Detach is used to remove this graph from the main graph\n",
    "    v_next_vals = critic.forward(next_states) #.detach()\n",
    "    v_curr_vals = critic.forward(states)\n",
    "    \n",
    "    # target_vec = torch.tensor(rewards) - torch.tensor(avg_reward) + (v_next_vals * (1 - torch.tensor(terminals)))\n",
    "    target_vec = torch.tensor(rewards) + (0.99 * v_next_vals * (1 - torch.tensor(terminals)))\n",
    "    estimate_vec = v_curr_vals\n",
    " \n",
    "\n",
    "    return target_vec, estimate_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Sarsa Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation} \n",
    "q_\\pi(s) \\approx \\hat{q}(s, a, w) \\doteq w^T x(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "## Section 2-2: Implement Agent Methods\n",
    "\n",
    "Let's first define methods that initialize the agent. `agent_init()` initializes all the variables that the agent will need.\n",
    "\n",
    "Now that we have implemented helper functions, let's create an agent. In this part, you will implement `agent_start()` and `agent_step()`. We do not need to implement `agent_end()` because there is no termination in our continuing task. \n",
    "\n",
    "`compute_softmax_prob()` is used in `agent_policy()`, which in turn will be used in `agent_start()` and `agent_step()`. We have implemented `agent_policy()` for you.\n",
    "\n",
    "When performing updates to the Actor and Critic, recall their respective updates in the Actor-Critic algorithm video.\n",
    "\n",
    "We approximate $q_\\pi$ in the Actor update using one-step bootstrapped return($R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, \\mathbf{w})$) subtracted by current state-value($\\hat{v}(S_{t}, \\mathbf{w})$), equivalent to TD error $\\delta$.\n",
    "\n",
    "$\\delta_t = R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_{t}, \\mathbf{w}) \\hspace{6em} (1)$\n",
    "\n",
    "**Average Reward update rule**: $\\bar{R} \\leftarrow \\bar{R} + \\alpha^{\\bar{R}}\\delta \\hspace{4.3em} (2)$\n",
    "\n",
    "**Critic weight update rule**: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^{\\mathbf{w}}\\delta\\nabla \\hat{v}(s,\\mathbf{w}) \\hspace{2.5em} (3)$\n",
    "\n",
    "**Actor weight update rule**: $\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\mathbf{\\theta}}\\delta\\nabla ln \\pi(A|S,\\mathbf{\\theta}) \\hspace{1.4em} (4)$\n",
    "\n",
    "\n",
    "However, since we are using linear function approximation and parameterizing a softmax policy, the above update rule can be further simplified using:\n",
    "\n",
    "$\\nabla \\hat{v}(s,\\mathbf{w}) = \\mathbf{x}(s) \\hspace{14.2em} (5)$\n",
    "\n",
    "$\\nabla ln \\pi(A|S,\\mathbf{\\theta}) = \\mathbf{x}_h(s,a) - \\sum_b \\pi(b|s, \\mathbf{\\theta})\\mathbf{x}_h(s,b) \\hspace{3.3em} (6)$\n",
    "\n",
    "For further details, see [Section 9.5.4 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=266). Image taken from the last reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)\n",
    "# SARSA\n",
    "class SarsaAgent():\n",
    "    \"\"\"\n",
    "    Initialization of Sarsa Agent. All values are set to None so they can\n",
    "    be initialized in the agent_init method.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.actor_step_size = None\n",
    "        self.critic_step_size = None\n",
    "        self.avg_reward_step_size = None\n",
    "\n",
    "        self.avg_reward = None\n",
    "        self.critic = None\n",
    "        self.actor = None\n",
    "\n",
    "        self.actions = None\n",
    "\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "    def agent_init(self, agent_config = {}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        # set step-size accordingly (we normally divide actor and critic step-size by num. tilings (p.217-218 of textbook))\n",
    "        self.actor_step_size = agent_config['optimizers_config']['actor_step_size']\n",
    "        self.critic_step_size = agent_config['optimizers_config']['critic_step_size']\n",
    "        self.avg_reward_step_size = agent_config['optimizers_config']['reward_step_size']\n",
    "\n",
    "        self.actions = agent_config['network_config']['num_actions']\n",
    "\n",
    "        # Set initial values of average reward, actor weights, and critic weights\n",
    "        # We initialize actor weights to three times the iht_size. \n",
    "        # Recall this is because we need to have one set of weights for each of the three actions.\n",
    "        self.avg_reward = 1.0\n",
    "        self.log_probs = 0\n",
    "        \n",
    "        self.actor = Actor(agent_config['network_config']).to(device)\n",
    "        self.critic = Critic(agent_config['network_config']).to(device)\n",
    "        \n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        self.actor_opti = optim.Adam(self.actor.parameters(), lr = self.actor_step_size ) \n",
    "        self.critic_opti = optim.Adam(self.critic.parameters(), lr = self.critic_step_size )\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon greedy\n",
    "        Args:\n",
    "        tiles - np.array, an array of active tiles\n",
    "        Returns:\n",
    "        (chosen_action, action_value) - (int, float), tuple of the chosen action\n",
    "                                        and it's value\n",
    "        \"\"\"\n",
    "        # Pass the states to create the normal distribution\n",
    "        dists = self.actor.forward(state)\n",
    "        # Sample an action from the current normal Distribution\n",
    "        action = dists.sample().detach().data.numpy()\n",
    "        #self.log_probs = dists.log_prob(torch.tensor(action).detach())\n",
    "        # Clip action to a given range\n",
    "        #m = nn.Tanh()\n",
    "        #chosen_action = m(action) * env.action_space.high.max()\n",
    "        chosen_action = np.clip(action, env.action_space.low.min(), env.action_space.high.max())\n",
    "\n",
    "        return [chosen_action]\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's env.reset() function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        current_action = self.select_action(state)\n",
    "\n",
    "        # Save action as last action\n",
    "        self.last_action = current_action\n",
    "        # Save tiles as previous tiles\n",
    "        self.last_state = state\n",
    "        \n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "                \n",
    "        td_target, td_estimate = get_td(self.last_state, state, reward, 0, self.actor, self.critic, self.avg_reward)\n",
    "\n",
    "        delta = td_target - td_estimate\n",
    "\n",
    "        #self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # zero the gradients buffer\n",
    "        self.critic_opti.zero_grad()\n",
    "        # Compute the  Mean squared value error loss\n",
    "        critic_loss = self.critic_loss(td_estimate.double().to(device), td_target.double().detach().to(device))\n",
    "        #critic_loss = delta**2\n",
    "        # Backprop the error\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_opti, 0.5)\n",
    "        # Optimize the network\n",
    "        self.critic_opti.step()\n",
    "        \n",
    "        # Actor \n",
    "        self.actor_opti.zero_grad()\n",
    "        # Compute mu and sigma\n",
    "        norm_dists = self.actor.forward(self.last_state)\n",
    "        # Construct equivalent loss function\n",
    "        logs_probs = norm_dists.log_prob(torch.tensor(self.last_action))\n",
    "        # Multiply by minus one as this is gradient ascent\n",
    "        #entropy = norm_dists.entropy()\n",
    "        actor_loss = -logs_probs * delta.detach()\n",
    "        #actor_loss = -logs_probs * delta\n",
    "        # Backprop the error\n",
    "        actor_loss.backward()\n",
    "        #(actor_loss + critic_loss).backward()\n",
    "        clip_grad_norm_(self.actor_opti, 0.5)\n",
    "        # Optimize the network\n",
    "        self.actor_opti.step()\n",
    "        #self.critic_opti.step()\n",
    "        \n",
    "        current_action = self.select_action(state)\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_action = current_action\n",
    "\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "\n",
    "        # There is no action_value used here because this is the end\n",
    "        # of the episode.\n",
    "        \n",
    "        state = np.zeros_like(self.last_state)\n",
    "        \n",
    "        \n",
    "        td_target, td_estimate = get_td(self.last_state, state, reward, 1, self.actor, self.critic, self.avg_reward)\n",
    "\n",
    "        delta = td_target - td_estimate\n",
    "\n",
    "        #self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # zero the gradients buffer\n",
    "        self.critic_opti.zero_grad()\n",
    "        # Compute the  Mean squared value error loss\n",
    "        critic_loss = self.critic_loss(td_estimate.double().to(device), td_target.double().detach().to(device))\n",
    "        # Backprop the error\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_opti, 0.5)\n",
    "        # Optimize the network\n",
    "        self.critic_opti.step()\n",
    "        \n",
    "        # Actor \n",
    "        self.actor_opti.zero_grad()\n",
    "        # Compute mu and sigma\n",
    "        norm_dists = self.actor.forward(self.last_state)\n",
    "        # Construct equivalent loss function\n",
    "        logs_probs = norm_dists.log_prob(torch.tensor(self.last_action))\n",
    "        # Multiply by minus one as this is gradient ascent\n",
    "        actor_loss = -logs_probs * delta.detach()\n",
    "        # Backprop the error\n",
    "        actor_loss.backward()\n",
    "        clip_grad_norm_(self.actor_opti, 0.5)\n",
    "        # Optimize the network\n",
    "        self.actor_opti.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "The following lines solves the Mountain Car problem and plot the average reward obtained over episodes and steps taken to solve the challenge at a specific episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:12<21:08, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 0 was: -9857.653080914199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:25<20:57, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 1 was: -8585.373776408758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:38<20:48, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 2 was: -9846.868831930376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [00:48<19:10, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 3 was: -9179.942439463983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [00:58<18:06, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 4 was: -8236.174689795154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [01:11<18:22, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 5 was: -8863.470058155028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [01:24<18:53, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 6 was: -8712.455066763052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [01:37<19:08, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 7 was: -9039.856749795586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [01:52<20:04, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 8 was: -9074.783085007612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [02:06<20:09, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 9 was: -7972.1799892443905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [02:20<20:24, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 10 was: -7716.00584942943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [02:33<19:28, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 11 was: -8786.563282071605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [02:47<19:53, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 12 was: -10056.308266432558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [02:58<18:12, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 13 was: -8050.182406615001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [03:09<17:14, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 14 was: -7762.480285431772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [03:20<16:33, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 15 was: -7908.106783715948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [03:33<17:02, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 16 was: -10254.705464119612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [03:46<17:06, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 17 was: -10110.967971138298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 19/100 [03:59<17:12, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 18 was: -7343.046399817438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 20/100 [04:14<17:33, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 19 was: -9295.382095025341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [04:28<17:44, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 20 was: -8640.852137956119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 22/100 [04:42<17:47, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 21 was: -7565.56505212768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [04:56<17:43, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 22 was: -7890.076577993322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 24/100 [05:07<16:23, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 23 was: -10767.977960578268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 25/100 [05:18<15:24, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 24 was: -8678.15595495255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [05:28<14:22, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 25 was: -7162.88158774846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [05:42<15:03, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 26 was: -8629.602969839909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [05:57<15:53, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 27 was: -7779.735329609214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 29/100 [06:12<16:16, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 28 was: -8739.676726250953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 30/100 [06:28<16:40, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 29 was: -8066.883284199606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 31/100 [06:39<15:29, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 30 was: -8674.92174786871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 32/100 [06:53<15:12, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 31 was: -8261.738720948233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 33/100 [07:07<15:11, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 32 was: -10016.993033398632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 34/100 [07:17<14:03, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward obtained during episode 33 was: -7641.426509086404\n"
     ]
    }
   ],
   "source": [
    "# Test the expected Sarsa Agent \n",
    "#model = ActionValueNetwork(network_config).to(device)\n",
    "num_runs = 1\n",
    "num_episodes = 100\n",
    "\n",
    "# Experiment parameters 256\n",
    "agent_info = {\n",
    "             'network_config': {\n",
    "                 'state_dim': env.observation_space.shape[0],\n",
    "                 'num_hidden_units': 256,\n",
    "                 'num_actions': env.action_space.shape[0]\n",
    "             },\n",
    "             'optimizers_config': {\n",
    "                 'actor_step_size': 5e-6,  #5e-6, 1e-5 \n",
    "                 'critic_step_size': 1e-5, \n",
    "                 'reward_step_size': 1e-6, \n",
    "             }}\n",
    "\n",
    "# Variable to store the amount of steps taken to solve the challeng\n",
    "all_steps = []\n",
    "# Variable to save the rewards in an episode\n",
    "all_rewards = []\n",
    "all_loss = []\n",
    "\n",
    "# Agent\n",
    "agent = SarsaAgent()\n",
    "\n",
    "# Environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "# Maximum number of possible iterations (default was 200)\n",
    "env._max_episode_steps = 1500\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "for n_runs in range(num_runs):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    # Reset agent\n",
    "    agent.agent_init(agent_info)\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "    # Steps, rewards and loss at each episode to solve the challenge\n",
    "    steps_per_episode = []\n",
    "    rewards_per_episode = []\n",
    "    loss_per_episode = []\n",
    "        \n",
    "    # Times the environment will start again without resetting the agent\n",
    "    for t in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # Reset done flag\n",
    "        done = False\n",
    "        # Set rewards, steps and loss to zero\n",
    "        rewards = 0\n",
    "        n_steps = 0\n",
    "        # Reset environment\n",
    "        observation = env.reset()\n",
    "        # Run until the experiment is over\n",
    "        while not done:\n",
    "            \n",
    "            # Render the environment only after t > # episodes\n",
    "            #if t > 95:\n",
    "            env.render()\n",
    "\n",
    "            # Take a step with the environment\n",
    "            observation, reward, done, info = env.step(last_action)\n",
    "            \n",
    "            rewards += reward\n",
    "            #n_steps += 1\n",
    "\n",
    "            # If the goal has been reached stop\n",
    "            if done:\n",
    "                # Last step with the agent\n",
    "                agent.agent_end(reward)\n",
    "            else:\n",
    "                # Take a step with the agent\n",
    "                last_action = agent.agent_step(reward, observation)\n",
    "                \n",
    "        # Append steps taken to solve the episode\n",
    "        #steps_per_episode.append(n_steps)\n",
    "        # Reward obtained during the episode\n",
    "        print(\"The reward obtained during episode {0} was: {1}\".format(t, rewards))\n",
    "        rewards_per_episode.append(rewards)\n",
    "        # Loss obtained solving the experiment\n",
    "        #loss_per_episode.append(agent.loss)\n",
    "\n",
    "    # Steps taken to solve the experiment during all\n",
    "    #all_steps.append(np.array(steps_per_episode))\n",
    "    # Awards obtained during all episode\n",
    "    #all_rewards.append(np.array(rewards_per_episode))\n",
    "    # Loss obtained during all episodes\n",
    "    #all_loss.append(loss_per_episode)\n",
    "\n",
    "env.close()\n",
    "print(np.mean(np.array(all_steps), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_average = np.mean(np.array(all_steps), axis=0)\n",
    "plt.plot(steps_average, label = 'Steps')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Iterations\",rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(steps_average.min(), steps_average.max())\n",
    "plt.title(\"Average iterations to solve the experiment over runs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The Minimum number of iterations used to solve the experiment were: {0}\\n\".format(np.array(all_steps).max()))\n",
    "print(\"The Maximum number of iterations used to solve the experiment were: {0}\\n\".format(np.array(all_steps).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_average = np.mean(all_rewards, axis=0)\n",
    "plt.plot(rewards_average, label = 'Average Reward')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\" ,rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(rewards_average.min(), rewards_average.max())\n",
    "plt.title(\"Average iterations to solve the experiment over runs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The best reward obtained solving the experiment was: {0}\\n\".format(np.array(all_rewards).max()))\n",
    "print(\"The Wordt reward obtained solving the experiment was: {0}\\n\".format(np.array(all_rewards).min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the last trained Agent \n",
    "\n",
    "This lines shows in a video the performance of the last trained agent and save a video with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test Sarsa Agent \n",
    "num_runs = 1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Environment\n",
    "env_to_wrap = gym.make('MountainCar-v0')\n",
    "# Maximum number of possible iterations (default was 200)\n",
    "env_to_wrap._max_episode_steps = 1500\n",
    "env = Monitor(env_to_wrap, \"./videos/mountainCar\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "for n_runs in tqdm(range(num_runs)):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "        \n",
    "    # Times the environment will start again without resetting the agent\n",
    "    for t in tqdm(range(num_episodes)):\n",
    "\n",
    "        # View environment\n",
    "        env.render()\n",
    "\n",
    "        # Take a step with the environment\n",
    "        observation, reward, done, info = env.step(last_action)\n",
    "\n",
    "        # If the goal has been reached stop\n",
    "        if done:\n",
    "            # Last step with the agent\n",
    "            agent.agent_end(reward)\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Take a step with the agent\n",
    "            last_action = agent.agent_step(reward, observation)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env_to_wrap.close()\n",
    "\n",
    "print(\"Episode finished after {} timesteps\".format(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Action-Values of the agent\n",
    "\n",
    "This final plot aims to show the action-values learned by the agent with Sarsa. The action value for a given state was calculated using: -$max_a\\hat{q}(s, a, w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution\n",
    "values = 500\n",
    "# Vector of positions\n",
    "pos_vals = np.linspace(-1.2, 0.5, num = values)\n",
    "# Vector of velocities\n",
    "vel_vals = np.linspace(-0.07, 0.07, num = values)\n",
    "\n",
    "# Z grid values\n",
    "av_grid = np.zeros((values, values))\n",
    "\n",
    "# Compute Action-values for each pos - vel pair\n",
    "for ix in range(len(pos_vals)):\n",
    "    for iy in range(len(vel_vals)):\n",
    "        av_grid[ix][iy] = -1 * agent.return_action_value([pos_vals[ix], vel_vals[iy]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3D surface\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "Px, Vy = np.meshgrid(pos_vals, vel_vals)\n",
    "ax.plot_surface(Vy, Px, av_grid, color = 'gray')\n",
    "ax.set_title(\"Cost-to-go function learned\", y = 1.1)\n",
    "ax.set_xlabel('Velocity')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_zlabel('Iterations')\n",
    "ax.view_init(45, azim=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
