{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodic Mountain Car function appoximation and control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is intended to solve the Episodic Lunar Lander problem using Semi-gradient Expected sarsa with neural networks for function approximation.\n",
    "\n",
    "The description of the problem is given below:\n",
    "\n",
    "\"Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\" \n",
    "\n",
    "<img src=\"./assets/lunar_lander.png\" width=\"380\" />\n",
    "\n",
    "Image and Text taken from [Official documentaiton Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process \n",
    "\n",
    "As a quick recap, the diagram below explains the workflow of a Markov Decision Process (MDP)\n",
    "\n",
    "<img src=\"./assets/MDP.png\" width=\"380\" />\n",
    "\n",
    "Image taken from [Section 3.1 Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Agent specifications\n",
    "\n",
    "The states, actions, reward and termination are given as follows for the lunar lander problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "\n",
    "     Type:  Box(8)\n",
    "     Num    Observation                           Min            Max\n",
    "     0      X position                            -inf           inf\n",
    "     1      Y position                            -inf           inf\n",
    "     2      X velocity                            -inf           inf\n",
    "     3      Y velocity                            -inf           inf\n",
    "     4      Theta w.r.t ground                    -inf           inf\n",
    "     5      Theta rate                            -inf           inf\n",
    "     6      1 if first leg has contact, else 0    -inf           inf\n",
    "     7      1 if second leg has contact, else 0   -inf           inf\n",
    "         \n",
    "**Actions**:\n",
    "\n",
    "     Type: Discrete(4)\n",
    "     Num    Action\n",
    "     0      Do nothing\n",
    "     1      Fire left engine\n",
    "     2      Fire main engine\n",
    "     3      Fire right engine\n",
    "\n",
    "        \n",
    "**Reward**:\n",
    "\n",
    "     Reward of 0 is awarded if the agent reached the flag(position = 0.5) on top of the mountain\n",
    "     Reward of -1 is awarded if the position of the agent is less than 0.5\n",
    "     Reward of -100 for flying off the screen\n",
    "     Reward of +100 for successful landing\n",
    "     Reward of -0.3 for firing main thrusters\n",
    "     Reward of -0.03 for firing side thrusters\n",
    "     Reward of +10 for each leg touching fround\n",
    "        \n",
    "**Starting State**:\n",
    "\n",
    "     The starting position is above the landing target\n",
    "        \n",
    "**Episode Termination**:\n",
    "\n",
    "     The lander crashes\n",
    "     The lander comes to rest\n",
    "     Episode length is greater than 200\n",
    "     \n",
    "For further information see [Github source code](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell aims to show how to iterate with the action and observation space of the agent and extract relevant information from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Action Space is an object of type: Discrete(4)\n",
      "\n",
      "The shape of the action space is: 4\n",
      "\n",
      "The Environment Space is an object of type: Box(8,)\n",
      "\n",
      "The Shape of the dimension Space are: (8,)\n",
      "\n",
      "The High values in the observation space are [inf inf inf inf inf inf inf inf], the low values are [-inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "\n",
      "The Observations at a given timestep are [ 0.07464371  0.9365864   0.57128644 -1.4850087  -0.98249376  1.7219391\n",
      "  1.0777016  -1.6392653 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation = env.reset() \n",
    "\n",
    "# Object's type in the action Space\n",
    "print(\"The Action Space is an object of type: {0}\\n\".format(env.action_space))\n",
    "# Shape of the action Space\n",
    "print(\"The shape of the action space is: {0}\\n\".format(env.action_space.n))\n",
    "# Object's type in the Observation Space\n",
    "print(\"The Environment Space is an object of type: {0}\\n\".format(env.observation_space))\n",
    "# Shape of the observation space\n",
    "print(\"The Shape of the dimension Space are: {0}\\n\".format(env.observation_space.shape))\n",
    "# The high and low values in the observation space\n",
    "print(\"The High values in the observation space are {0}, the low values are {1}\\n\".format(\n",
    "    env.observation_space.high, env.observation_space.low))\n",
    "# Example of observation\n",
    "print(\"The Observations at a given timestep are {0}\\n\".format(env.observation_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing action-values with neural networks\n",
    "\n",
    "To compute action-values, a feed-forward neural network is used. This apporach allows us to compute action-values using the weights of the neural network.\n",
    "\n",
    "$$ q_\\pi(s) \\approx \\hat{q}(s, a, w) = NN(s,a,w) $$\n",
    "\n",
    "Neural networks are used to solve the control problem in RL, particularly, this networl is going to be used with an Episodic Semi-gradient Expected Sarsa agent. The inputs of the network are the states, which in this case are eight, the number of hidden layers and hidden units can vary. Finally, the number of inputs is equals to the number of actions in the problem, therefore, four output nodes are needed in the final layer. Each output node corresponds to the action value of a particular action.\n",
    "\n",
    "<img src=\"./assets/nn.png\" width=\"320\" />\n",
    "\n",
    "\n",
    "For further information about Neural Networks for function approximation see [Section 9.7 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=246)\n",
    "\n",
    "Image taken from [Reinforcement learning specialization, C4L5S1](https://www.coursera.org/learn/complete-reinforcement-learning-system/lecture/CVH40/meeting-with-adam-getting-the-agent-details-right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Netork to compute action values\n",
    "class ActionValueNetwork(nn.Module):\n",
    "    # Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).\n",
    "    def __init__(self, network_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of states\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        # Hidden units\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        # Actions or output units\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        \n",
    "        # Initialzie hidden layer \n",
    "        self.hidden = nn.Linear(self.state_dim, self.num_hidden_units)\n",
    "        # Initialize output layer\n",
    "        self.output = nn.Linear(self.num_hidden_units, self.num_actions)\n",
    "                \n",
    "    \n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        This is a feed-forward pass in the network\n",
    "        Args:\n",
    "            s (Numpy array): The state, a 2D array of shape (batch_size, state_dim)\n",
    "        Returns:\n",
    "            The action-values (Torch array) calculated using the network's weights.\n",
    "            A 2D array of shape (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # Transform observations into a pytorch tensor\n",
    "        s = torch.Tensor(s)\n",
    "        \n",
    "        q_vals = F.relu(self.hidden(s))\n",
    "        q_vals = self.output(q_vals)\n",
    "\n",
    "        return q_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "\n",
    "Experience replay is a technique very similar to planning in RL. Overall, this technique is used to update the action-values of the agent with a set of \"experience\" collected in a model. This experience allows the model to learn without interacting with the environment using simulated experience.\n",
    "\n",
    "Experience replay is a simple method that can get some of the advantages of planning by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. The data stored in the model are the state, action, reward and next state. \n",
    "\n",
    "The model will be filled until a queue size is reached, only then the model will drop its oldest observation and add a new one. With this buffer of information, it is possible to sample \"batches\" and update the action values of the agent.\n",
    "\n",
    "As a quick recap, the next pseudocode shows the pseudocode for Dyna-Q algorithm where the agent performs planning steps, improving the learning process of the agent with simulated experience.\n",
    "\n",
    "<img src=\"./assets/dyna-q.png\" width=\"480\" />\n",
    "\n",
    "The planning process is given in the next image.\n",
    "\n",
    "<img src=\"./assets/planning.png\" width=\"320\" />\n",
    "\n",
    "For further information about planning see [Section 8.2 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=185). **Note**: Images taken from the last reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        \"\"\"\n",
    "        # Create the buffer\n",
    "        self.buffer = [] \n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state of size (state_dim)            \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state of size (state_dim) .           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            # Delete first position of the buffer if the Queue size is equals to max size\n",
    "            del self.buffer[0]\n",
    "        # Append new step\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "            The return of this function is of size (minibatch_size)\n",
    "        \"\"\"\n",
    "        idxs = np.random.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Policy\n",
    "\n",
    "To compute the actions, a softmax policy is used. One advantage of a softmax policy is that it explores according to the action-values, meaning that an action with a moderate value has a higher chance of getting selected compared to an action with a lower value. This sort of policies provides a feasible alternative to do exploration.\n",
    "\n",
    "The probability of selecting each action according to the softmax policy is shown below:\n",
    "\n",
    "$$Pr{(A_t=a | S_t=s)} \\hspace{0.1cm} \\dot{=} \\hspace{0.1cm} \\frac{e^{Q(s, a)/\\tau}}{\\sum_{b \\in A}e^{Q(s, b)/\\tau}}$$\n",
    "\n",
    "Here, $\\tau$ is the temperature parameter which controls how much the agent focuses on the highest valued actions. The smaller the temperature, the more the agent selects the greedy action. Conversely, when the temperature is high, the agent selects among actions more uniformly random.\n",
    "\n",
    "Given that a softmax policy exponentiates action values, if those values are large, exponentiating them could get very large. To implement the softmax policy in a numerically stable way,the maximum action-value is substracted from the action-values. Doing so, the probability of selecting each action looks as follows:\n",
    "\n",
    "$$Pr{(A_t=a | S_t=s)} \\hspace{0.1cm} \\dot{=} \\hspace{0.1cm} \\frac{e^{Q(s, a)/\\tau - max_{c}Q(s, c)/\\tau}}{\\sum_{b \\in A}e^{Q(s, b)/\\tau - max_{c}Q(s, c)/\\tau}}$$\n",
    "\n",
    "Recall that changing the action preferences (action-values in this case) for a constant, would not change the final value of the softmax probability. This Softmax implementation is different than the one provided by Pytorch.\n",
    "\n",
    "For further informartion about Softmax policies and action preferences see [Section 13.1 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=344)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Tensor array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D Tensor array of shape (batch_size, num_actions). Where each column is a probability distribution \n",
    "        over the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences\n",
    "    preferences = action_values / tau\n",
    "    # Compute the maximum preference across the actions (Max action per row or batch)\n",
    "    max_preference = torch.max(preferences, dim = 1)[0]\n",
    "\n",
    "    # Reshape max_preference  to [Batch, 1] \n",
    "    reshaped_max_preference = max_preference.view((-1, 1))\n",
    "    \n",
    "    # Computing numerator\n",
    "    exp_preferences = torch.exp(preferences - reshaped_max_preference)\n",
    "    # Computing the denominator suming over rows (batches).\n",
    "    sum_of_exp_preferences = torch.sum(exp_preferences, dim = 1)\n",
    "    \n",
    "    # Reshape sum_of_exp_preferences array to [Batch, 1] \n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.view((-1, 1))\n",
    "    \n",
    "    # Computing action probabilities\n",
    "    action_probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
    "    action_probs = action_probs.squeeze()\n",
    "    \n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs tensor([[0.2585, 0.0169, 0.0537, 0.6709],\n",
      "        [0.8470, 0.0029, 0.1352, 0.0149]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Testing the Softmax implementation\n",
    "rand_generator = np.random.RandomState(0)\n",
    "action_values = torch.tensor(rand_generator.normal(0, 1, (2, 4)))\n",
    "tau = 0.5\n",
    "\n",
    "action_probs = softmax(action_values, tau)\n",
    "print(\"action_probs\", action_probs)\n",
    "\n",
    "assert(np.allclose(action_probs, np.array([\n",
    "    [0.25849645, 0.01689625, 0.05374514, 0.67086216],\n",
    "    [0.84699852, 0.00286345, 0.13520063, 0.01493741]\n",
    "])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TD target and TD estimate\n",
    "\n",
    "The TD target and TD estimate's computation will be done in the next lines. The main idea here is to obtain the action-value network updates with experience sampled from the experience replay buffer.\n",
    "\n",
    "At time $t$, there is an action-value function represented as a neural network, say $Q_t$. The idea is to update the action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at.\n",
    "\n",
    "In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current \"un-updated\" action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& Q_t \\leftarrow \\text{action-value network at timestep t (current action-value network)}\\\\\n",
    "& \\text{Initialize } Q_{t+1}^1 \\leftarrow Q_t\\\\\n",
    "& \\text{For } i \\text{ in } [1, ..., N] \\text{ (i.e. N} \\text{  replay steps)}:\\\\\n",
    "& \\hspace{1cm} s, a, r, t, s'\n",
    "\\leftarrow \\text{Sample batch of experiences from experience replay buffer} \\\\\n",
    "& \\hspace{1cm} \\text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \\leftarrow Q_{t+1}^{i}(s, a) + \\alpha \\cdot \\left[r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) - Q_{t+1}^{i}(s, a)\\right]\\\\\n",
    "& \\hspace{1.5cm} \\text{ making sure to add the } \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) \\text{ for non-terminal transitions only.} \\\\\n",
    "& \\text{After N replay steps, we set } Q_{t+1}^{N} \\text{ as } Q_{t+1} \\text{ and have a new } Q_{t+1} \\text{for time step } t + 1 \\text{ that we will fix in the next set of updates. }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. \n",
    "\n",
    "$$ R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, w)- \\hat{q}(S_t, A_t, w) $$\n",
    " \n",
    "Recall that the for this problem, the TD Target is given by.\n",
    "\n",
    "$$ r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) $$\n",
    "\n",
    "Similarly, the TD estimate is.\n",
    "\n",
    "$$ Q_{t+1}^{i}(s, a) $$\n",
    "\n",
    "The Semi-gradient Expected Sarsa update is given below.\n",
    "\n",
    "$$w \\leftarrow w + \\alpha[R_{t+1} + \\gamma \\sum_{a'}\\pi(a' | S_{t+1}) \\hat{q}(S_{t+1}, a', w) - \\hat{q}(S_t, A_t, w)]\\nabla \\hat{q}(S_t, A_t, w)$$\n",
    "\n",
    "\n",
    "For further explanation about Episodic semi-gradient control see [Section 10.1 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=265)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute the TD Target and TD estimate\n",
    "def get_td(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor (gamma).\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        target_vec (Tensor array): The TD Target for actions taken, of shape (batch_size,)\n",
    "        estimate_vec (Tensor array): The TD estimate for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used \n",
    "    # for computing the  targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # q_next_mat is a 2D Tensor of shape (batch_size, num_actions)\n",
    "    # used to compute the action-values of the next states\n",
    "    # Detach is used to remove this graph from the main graph\n",
    "    q_next_mat = current_q.forward(next_states).detach()\n",
    "    \n",
    "    # Compute policy at next state. \n",
    "    # probs_mat is a 2D Tensor of shape (batch_size, num_actions)\n",
    "    probs_mat = softmax(q_next_mat, tau)\n",
    "    \n",
    "    # Sum of the action-values for the next_states weighted by the policy, probs_mat.\n",
    "    # (1 - terminals) to make sure v_next_vec is zero for terminal next states.\n",
    "    # v_next_vec is a 1D Tensor of shape (batch_size,)\n",
    "    v_next_vec = torch.zeros((q_next_mat.shape[0]), dtype=torch.float64).detach()\n",
    "    # Sum over rows axis (batches)\n",
    "    v_next_vec = torch.sum(probs_mat * q_next_mat, dim = 1) * (1 - torch.tensor(terminals))    \n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # target_vec is a 1D Tensor of shape (batch_size,)\n",
    "    target_vec = torch.tensor(rewards) + (discount * v_next_vec)\n",
    "    \n",
    "    # Computing action values at the current states for all actions using network\n",
    "    # q_mat is a 2D Tensor of shape (batch_size, num_actions)\n",
    "    q_mat = network.forward(states)\n",
    "    \n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = torch.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # q_vec are the estimates\n",
    "    # q_vec is a 1D Tensor of shape (batch_size)\n",
    "    estimate_vec = q_mat[batch_indices, actions]    \n",
    "\n",
    "    return target_vec, estimate_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Network's optmizer\n",
    "\n",
    "One important step is to optimize the network using the TD estimate and the TD target computed previously. As a quick recap, the Mean squared value error is given below.\n",
    "\n",
    "$$\\overline{VE} = \\sum_s\\mu(s)[V_\\pi(s) - \\hat{v}(s,w)]^2$$\n",
    "\n",
    "The idea is to use $\\overline{VE}$ as the Loss function to optimize the action-value network. For this particular problem, the MSE implementation provided by Pytorch is used. Additionally, the Adam optimizer is used to optimize the weights of the neural network.\n",
    "\n",
    "See [Section 9.2 of Reinforment Learning an Introduction](http://www.incompleteideas.net/book/RLbook2018.pdf#page=221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Work Required: Yes. Fill in code in optimize_network (~2 Lines).\n",
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau, criterion):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Return:\n",
    "        Loss (float): The loss value for the current batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states) # Batch per states\n",
    "    next_states = np.concatenate(next_states) # Batch per states\n",
    "    rewards = np.array(rewards) # Batch size\n",
    "    terminals = np.array(terminals) # Batch size\n",
    "    batch_size = states.shape[0] # Batch size\n",
    "    \n",
    "    # Computing TD target and estimate using get_td function\n",
    "    td_target, td_estimate = get_td(states, next_states, actions, rewards, discount, terminals, \\\n",
    "                                          network, current_q, tau)\n",
    "    \n",
    "    # zero the gradients buffer\n",
    "    optimizer.zero_grad()\n",
    "    # Compute the  Mean squared value error loss\n",
    "    loss = criterion(td_estimate.double().to(device), td_target.to(device))\n",
    "    # Backprop the error\n",
    "    loss.backward()\n",
    "    # Optimize the network\n",
    "    optimizer.step()\n",
    "    \n",
    "    return (loss / batch_size).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Expected-Sarsa Agent\n",
    "\n",
    "The final step is to use all the methods implemented above in the Expected-Sarsa Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Expected Expected-Sarsa Agent\n",
    "class ExpectedSarsaAgent():\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'])\n",
    "        # Add model to CPU or GPU respectively\n",
    "        self.network = ActionValueNetwork(agent_config['network_config']).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr = agent_config['optimizer_config']['step_size'], \n",
    "                                    betas=(agent_config['optimizer_config']['beta_m'], agent_config['optimizer_config']['beta_v']),\n",
    "                                    eps=agent_config['optimizer_config']['epsilon']) \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "                \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.loss = 0\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        action_values = self.network.forward(state)\n",
    "        probs_batch = softmax(action_values, self.tau).detach().numpy()\n",
    "        action = np.random.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add current reward to the sum of rewards\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the forward() and get_td() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state) #change for state for submission, normally, it is self.last_state\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            # Make a copy of the current network to obtain stable targets\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                self.loss +=optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau,\n",
    "                                 self.criterion)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        return self.last_action\n",
    "\n",
    "    # update of the weights using optimize_network \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                self.loss += optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau,\n",
    "                                 self.criterion)\n",
    "                \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards, self.episode_steps\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "The following lines solves the Lunar Lander problem and plot the average reward obtained over episodes, steps taken to solve the challenge at a specific episode and average loss over episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 992/1000 [53:23<00:17,  2.20s/it]   "
     ]
    }
   ],
   "source": [
    "# Test the expected Sarsa Agent \n",
    "#model = ActionValueNetwork(network_config).to(device)\n",
    "num_runs = 1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Experiment parameters\n",
    "agent_info = {\n",
    "             'network_config': {\n",
    "                 'state_dim': env.observation_space.shape[0],\n",
    "                 'num_hidden_units': 256,\n",
    "                 'num_actions': env.action_space.n\n",
    "             },\n",
    "             'optimizer_config': {\n",
    "                 'step_size': 1e-3, \n",
    "                 'beta_m': 0.9, \n",
    "                 'beta_v': 0.999,\n",
    "                 'epsilon': 1e-8\n",
    "             },\n",
    "             'replay_buffer_size': 50000,\n",
    "             'minibatch_sz': 8,\n",
    "             'num_replay_updates_per_step': 4,\n",
    "             'gamma': 0.99,\n",
    "             'tau': 0.001}\n",
    "\n",
    "# Variable to store the amount of steps taken to solve the challeng\n",
    "all_steps = []\n",
    "# Variable to save the rewards in an episode\n",
    "all_rewards = []\n",
    "all_loss = []\n",
    "\n",
    "# Agent\n",
    "agent = ExpectedSarsaAgent()\n",
    "\n",
    "# Environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "# Maximum number of possible iterations (default was 200)\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "for n_runs in range(num_runs):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    # Reset agent\n",
    "    agent.agent_init(agent_info)\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "    # Steps, rewards and loss at each episode to solve the challenge\n",
    "    steps_per_episode = []\n",
    "    rewards_per_episode = []\n",
    "    loss_per_episode = []\n",
    "        \n",
    "    # Times the environment will start again without resetting the agent\n",
    "    for t in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # Reset done flag\n",
    "        done = False\n",
    "        # Set rewards, steps and loss to zero\n",
    "        rewards = 0\n",
    "        n_steps = 0\n",
    "        agent.loss = 0\n",
    "        # Reset environment\n",
    "        observation = env.reset()\n",
    "        # Run until the experiment is over\n",
    "        while not done:\n",
    "            \n",
    "            # Render the environment only after t > # episodes\n",
    "            if t > 295:\n",
    "                env.render()\n",
    "\n",
    "            # Take a step with the environment\n",
    "            observation, reward, done, info = env.step(last_action)\n",
    "            \n",
    "            rewards += reward\n",
    "            n_steps += 1\n",
    "\n",
    "            # If the goal has been reached stop\n",
    "            if done:\n",
    "                # Last step with the agent\n",
    "                agent.agent_end(reward)\n",
    "            else:\n",
    "                # Take a step with the agent\n",
    "                last_action = agent.agent_step(reward, observation)\n",
    "                \n",
    "        # Append steps taken to solve the episode\n",
    "        steps_per_episode.append(n_steps)\n",
    "        # Reward obtained during the episode\n",
    "        rewards_per_episode.append(rewards)\n",
    "        # Loss obtained solving the experiment\n",
    "        loss_per_episode.append(agent.loss)\n",
    "\n",
    "    # Steps taken to solve the experiment during all\n",
    "    all_steps.append(np.array(steps_per_episode))\n",
    "    # Awards obtained during all episode\n",
    "    all_rewards.append(np.array(rewards_per_episode))\n",
    "    # Loss obtained during all episodes\n",
    "    all_loss.append(loss_per_episode)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_average = np.mean(np.array(all_steps), axis=0)\n",
    "plt.plot(steps_average, label = 'Steps')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Iterations\",rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(steps_average.min(), steps_average.max())\n",
    "plt.title(\"Average iterations to solve the experiment over runs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The Minimum number of iterations used to solve the experiment were: {0}\\n\".format(np.array(all_steps).min()))\n",
    "print(\"The Maximum number of iterations used to solve the experiment were: {0}\\n\".format(np.array(all_steps).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_average = np.mean(all_rewards, axis=0)\n",
    "plt.plot(rewards_average, label = 'Average Reward')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\" ,rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(rewards_average.min(), rewards_average.max())\n",
    "plt.title(\"Average reward to solve the experiment over runs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The best reward obtained solving the experiment was: {0}\\n\".format(np.array(all_rewards).max()))\n",
    "print(\"The Worst reward obtained solving the experiment was: {0}\\n\".format(np.array(all_rewards).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_average = np.mean(np.array(all_loss), axis=0)\n",
    "plt.plot(loss_average, label = 'Steps')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average loss\",rotation=0, labelpad=40)\n",
    "plt.xlim(-0.2, num_episodes)\n",
    "plt.ylim(loss_average.min(), loss_average.max())\n",
    "plt.title(\"Average loss over iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"The best loss obtained solving the experiment was: {0}\\n\".format(np.array(loss_average).min()))\n",
    "print(\"The Worst loss obtained solving the experiment was: {0}\\n\".format(np.array(loss_average).max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the last trained Agent \n",
    "\n",
    "This lines shows in a video the performance of the last trained agent and save a video with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test Sarsa Agent \n",
    "num_runs = 1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Environment\n",
    "env_to_wrap = gym.make('LunarLander-v2')\n",
    "# Maximum number of possible iterations (default was 200)\n",
    "env_to_wrap._max_episode_steps = 1500\n",
    "env = Monitor(env_to_wrap, \"./videos/lunarLander\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "\n",
    "# Number of runs are the times the experiment will start again (a.k.a episode)\n",
    "for n_runs in tqdm(range(num_runs)):\n",
    "    \n",
    "    # Resets environment\n",
    "    observation = env.reset()\n",
    "    # Generate last state and action in the agent\n",
    "    last_action = agent.agent_start(observation)\n",
    "        \n",
    "    # Times the environment will start again without resetting the agent\n",
    "    for t in tqdm(range(num_episodes)):\n",
    "\n",
    "        # View environment\n",
    "        env.render()\n",
    "        # Take a step with the environment\n",
    "        observation, reward, done, info = env.step(last_action)\n",
    "\n",
    "        # If the goal has been reached stop\n",
    "        if done:\n",
    "            # Last step with the agent\n",
    "            agent.agent_end(reward)\n",
    "            break\n",
    "        else:\n",
    "            # Take a step with the agent\n",
    "            last_action = agent.agent_step(reward, observation)\n",
    "\n",
    "env.close()\n",
    "env_to_wrap.close()\n",
    "\n",
    "print(\"Episode finished after {} timesteps\".format(t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
